{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to NNfluxnu","text":"<p>The NNfluxnu package is a code based on NNPDF to fit neutrino PDFs using neutrino scattering events at the LHC. Using so-called closure tests, one can parametrise a neutrino flux using a feed-forward NN to make a theory agnostic parametrisation using pseudo data or event rate measurements from FASER/SND@LHC. </p>"},{"location":"#installation","title":"Installation","text":"<p>First make a conda environment with python=3.10 <pre><code>conda create -n nnfluxnu python=3.10\nconda activate nnfluxnu\n</code></pre> Then install poetry using either pip: <pre><code>curl -sSL https://install.python-poetry.org | python3 -\n</code></pre></p> <p>or pipx: <pre><code>pipx install poetry\n</code></pre></p> <p>The code can directly be installed from the git repository using: <pre><code>git clone https://github.com/JukkaJohn/nnfluxnu.git\ncd nnfluxnu\npoetry install\n</code></pre> The LHAPDF library could not be added to poetry, so this should be added seperately by running: <pre><code>conda install -c conda-forge lhapdf\n</code></pre> This will create a folder in conda_dir/envs/nnfluxnu/share/LHAPDF All neutrino PDFs in the LHAPDF format should be read and written to here. So: <pre><code>cp -r neutrino_pdfs_lhpadf/* conda_path/envs/nnfluxnu/share/LHAPDF\n</code></pre></p>"},{"location":"#project-description","title":"Project Description","text":"<p>This code was written as part of a Master's project at the University of Amsterdam, Vrije Universiteit Amsterdam and Nikhef. </p> <p>In this work, theory agnostic parametrisations of neutrino fluxes are made using feed-forward neural networks. The paper based on this code and project can be read here (insert link) and the master's thesis can be read  here. In order to fit the neutrino fluxes, actual DIS charged current event rate measurements from the FASER collaboration are used as well as pseudo data generated using several MC event generators simulating forward hadron production at the LHC. The neutrino flux is related to the event rate measurements by several integrals. These are replaced by so-called FK-tables which encapsulate all the information on DIS structure functions and are contained in a matrix. These FK-tables improve the computational efficiency significantly. This code can fit both pseudo data and event rate measurements from faser to fit electron and muon neutrinos. All the neutrino fluxes parametrised used in this work are written to LHAPDF grids and can also be found in the git repository. The code was also used to research several physics applications by comparing neutrino fluxes from different observables, MC generators, detector geometries, enhanced BSM decays and the influence of IC on neutrino fluxes. </p> <p>The structure of the code can be found in the Framework section. How one should use the code is explained in the Usage section. </p>"},{"location":"api/","title":"\ud83d\udcd8 API Reference","text":"<p>Welcome to the full API reference for the <code>NN_fit</code> package. Below you\u2019ll find organized documentation for all modules in the codebase, including classes, functions, and docstrings.</p>"},{"location":"api/#core-fitting-modules","title":"\ud83d\udd27 Core Fitting Modules","text":""},{"location":"api/#perform_fit_combpy","title":"<code>perform_fit_comb.py</code>","text":""},{"location":"api/#NN_fit.perform_fit_comb.perform_fit","title":"<code>perform_fit(pred, num_reps, range_alpha, range_beta, range_gamma, lr, wd, patience, x_alphas, fk_tables, binwidths, cov_matrix, extended_loss, activation_function, num_input_layers, num_output_layers, hidden_layers, x_vals, preproc, validation_split, max_epochs, max_chi_sq, lag_mult_pos, lag_mult_int, x_int)</code>","text":"<p>Performs repeated training of neural networks to fit pseudo-data predictions using a physics-constrained loss function and neural PDF parameterization.</p> <p>Each replica (<code>num_reps</code>) of the prediction is fitted using a randomly initialized neural network (based on PreprocessedMLP), and the corresponding predictions and losses are recorded. Optionally uses a validation split.</p>"},{"location":"api/#NN_fit.perform_fit_comb.perform_fit--parameters","title":"Parameters","text":"<p>pred : list of np.ndarray     List of length <code>num_reps</code>, each containing the pseudo-data event predictions. num_reps : int     Number of replicas (i.e., independent fits with random initialization). range_alpha : float     Upper bound for uniform sampling of <code>alpha</code> preprocessing parameter. range_beta : float     Upper bound for uniform sampling of <code>beta</code> preprocessing parameter. range_gamma : float     Upper bound for uniform sampling of <code>gamma</code> preprocessing parameter. lr : float     Learning rate for the Adam optimizer. wd : float     Weight decay (L2 regularization) for the optimizer. patience : int     Early stopping patience (currently unused but declared). x_alphas : torch.Tensor     Input x-values used to evaluate PDF predictions for data loss. fk_tables : torch.Tensor     FastKernel tables to convert PDFs into observable space. binwidths : torch.Tensor     Widths of each bin used in rebinning the predictions. cov_matrix : np.ndarray     Covariance matrix used for weighted loss calculation. extended_loss : bool     If True, uses extended loss with constraints (e.g., normalization, positivity). activation_function : str     Activation function used in the neural network (e.g., 'relu', 'tanh'). num_input_layers : int     Number of input layers before hidden layers. num_output_layers : int     Number of output layers after hidden layers. hidden_layers : list of int     Number of neurons in each hidden layer. x_vals : np.ndarray     x-values used to store final fitted PDFs. preproc : str     Type of preprocessing function (e.g., 'powerlaw', 'exp') applied to PDFs. validation_split : float     Fraction of data used for validation (between 0 and 1). max_epochs : int     Maximum number of training epochs. max_chi_sq : float     Maximum allowed chi-squared for a fit to be accepted. lag_mult_pos : float     Lagrange multiplier for positivity constraint. lag_mult_int : float     Lagrange multiplier for integral constraint. x_int : np.ndarray     x-values used for computing integral constraints.</p>"},{"location":"api/#NN_fit.perform_fit_comb.perform_fit--returns","title":"Returns","text":"<p>chi_squares : list of float     Training loss (chi-squared) values saved periodically during training. N_event_pred : list of np.ndarray     Predicted event yields after applying the FastKernel convolution. neutrino_pdfs : list of np.ndarray     Final predicted PDFs (postprocessed) from successful fits. model : PreprocessedMLP     Final trained model (from last accepted fit). chi_square_for_postfit : list of float     Final chi-squared values for each accepted fit (for post-fit evaluation). train_indices : np.ndarray     Indices used for training set in the last run (if validation was used). val_indices : np.ndarray     Indices used for validation set in the last run (if validation was used). training_length : int     Number of training steps run in the final (last) model.</p>"},{"location":"api/#NN_fit.perform_fit_comb.perform_fit--notes","title":"Notes","text":"<ul> <li>Only models with <code>loss &lt; max_chi_sq</code> are retained in the final output.</li> <li>The PDFs are preprocessed using a parameterized function with random \u03b1, \u03b2, \u03b3 values.</li> <li>Assumes the model class <code>PreprocessedMLP</code> and loss class <code>CustomLoss</code> are defined externally.</li> <li>Model and predictions use PyTorch; inputs must be tensors where appropriate.</li> <li>Currently no explicit early stopping logic is implemented (but patience is reserved).</li> </ul> Source code in <code>NN_fit/perform_fit_comb.py</code> <pre><code>def perform_fit(\n    pred: List[np.ndarray],\n    num_reps: int,\n    range_alpha: float,\n    range_beta: float,\n    range_gamma: float,\n    lr: float,\n    wd: float,\n    patience: int,\n    x_alphas: torch.Tensor,\n    fk_tables: torch.Tensor,\n    binwidths: torch.Tensor,\n    cov_matrix: np.ndarray,\n    extended_loss: bool,\n    activation_function: str,\n    num_input_layers: int,\n    num_output_layers: int,\n    hidden_layers: List[int],\n    x_vals: np.ndarray,\n    preproc: str,\n    validation_split: float,\n    max_epochs: int,\n    max_chi_sq: float,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n    x_int: np.ndarray,\n) -&gt; Tuple[\n    List[float],  # chi_squares\n    List[np.ndarray],  # N_event_pred\n    List[np.ndarray],  # neutrino_pdfs\n    PreprocessedMLP,  # model (last accepted fit)\n    List[float],  # chi_square_for_postfit\n    np.ndarray,  # train_indices\n    np.ndarray,  # val_indices\n    int,  # training_length\n]:\n    \"\"\"\n    Performs repeated training of neural networks to fit pseudo-data predictions using a\n    physics-constrained loss function and neural PDF parameterization.\n\n    Each replica (`num_reps`) of the prediction is fitted using a randomly initialized\n    neural network (based on PreprocessedMLP), and the corresponding predictions and losses\n    are recorded. Optionally uses a validation split.\n\n    Parameters\n    ----------\n    pred : list of np.ndarray\n        List of length `num_reps`, each containing the pseudo-data event predictions.\n    num_reps : int\n        Number of replicas (i.e., independent fits with random initialization).\n    range_alpha : float\n        Upper bound for uniform sampling of `alpha` preprocessing parameter.\n    range_beta : float\n        Upper bound for uniform sampling of `beta` preprocessing parameter.\n    range_gamma : float\n        Upper bound for uniform sampling of `gamma` preprocessing parameter.\n    lr : float\n        Learning rate for the Adam optimizer.\n    wd : float\n        Weight decay (L2 regularization) for the optimizer.\n    patience : int\n        Early stopping patience (currently unused but declared).\n    x_alphas : torch.Tensor\n        Input x-values used to evaluate PDF predictions for data loss.\n    fk_tables : torch.Tensor\n        FastKernel tables to convert PDFs into observable space.\n    binwidths : torch.Tensor\n        Widths of each bin used in rebinning the predictions.\n    cov_matrix : np.ndarray\n        Covariance matrix used for weighted loss calculation.\n    extended_loss : bool\n        If True, uses extended loss with constraints (e.g., normalization, positivity).\n    activation_function : str\n        Activation function used in the neural network (e.g., 'relu', 'tanh').\n    num_input_layers : int\n        Number of input layers before hidden layers.\n    num_output_layers : int\n        Number of output layers after hidden layers.\n    hidden_layers : list of int\n        Number of neurons in each hidden layer.\n    x_vals : np.ndarray\n        x-values used to store final fitted PDFs.\n    preproc : str\n        Type of preprocessing function (e.g., 'powerlaw', 'exp') applied to PDFs.\n    validation_split : float\n        Fraction of data used for validation (between 0 and 1).\n    max_epochs : int\n        Maximum number of training epochs.\n    max_chi_sq : float\n        Maximum allowed chi-squared for a fit to be accepted.\n    lag_mult_pos : float\n        Lagrange multiplier for positivity constraint.\n    lag_mult_int : float\n        Lagrange multiplier for integral constraint.\n    x_int : np.ndarray\n        x-values used for computing integral constraints.\n\n    Returns\n    -------\n    chi_squares : list of float\n        Training loss (chi-squared) values saved periodically during training.\n    N_event_pred : list of np.ndarray\n        Predicted event yields after applying the FastKernel convolution.\n    neutrino_pdfs : list of np.ndarray\n        Final predicted PDFs (postprocessed) from successful fits.\n    model : PreprocessedMLP\n        Final trained model (from last accepted fit).\n    chi_square_for_postfit : list of float\n        Final chi-squared values for each accepted fit (for post-fit evaluation).\n    train_indices : np.ndarray\n        Indices used for training set in the last run (if validation was used).\n    val_indices : np.ndarray\n        Indices used for validation set in the last run (if validation was used).\n    training_length : int\n        Number of training steps run in the final (last) model.\n\n    Notes\n    -----\n    - Only models with `loss &lt; max_chi_sq` are retained in the final output.\n    - The PDFs are preprocessed using a parameterized function with random \u03b1, \u03b2, \u03b3 values.\n    - Assumes the model class `PreprocessedMLP` and loss class `CustomLoss` are defined externally.\n    - Model and predictions use PyTorch; inputs must be tensors where appropriate.\n    - Currently no explicit early stopping logic is implemented (but patience is reserved).\n    \"\"\"\n    (\n        neutrino_pdfs,\n        N_event_pred,\n        chi_squares,\n        preproc_pdfs,\n        nn_pdfs,\n        chi_square_for_postfit,\n        val_losses,\n    ) = [], [], [], [], [], [], []\n    x_vals = torch.tensor(x_vals, dtype=torch.float32).view(-1, 1)\n    x_int = torch.tensor(x_int, dtype=torch.float32).view(-1, 1)\n\n    for i in range(num_reps):\n        training_length = 0\n        counter = 0\n        best_loss = 1e13  # initial loss\n        alpha, beta, gamma = (\n            np.random.rand() * range_alpha,\n            np.random.rand() * range_beta,\n            np.random.rand() * range_gamma,\n        )\n\n        model = PreprocessedMLP(\n            alpha,\n            beta,\n            gamma,\n            activation_function,\n            hidden_layers,\n            num_input_layers,\n            num_output_layers,\n            preproc,\n        )\n\n        criterion = CustomLoss(\n            extended_loss,\n            num_output_layers,\n        )\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\n        dataset_size = pred[i].shape[0]\n        indices = np.arange(dataset_size)\n\n        np.random.shuffle(indices)\n        val_size = int(dataset_size * validation_split)\n\n        train_indices, val_indices = indices[val_size:], indices[:val_size]\n\n        if validation_split != 0:\n            pred_train = pred[i][train_indices]\n            cov_matrix_train = cov_matrix[train_indices][:, train_indices]\n            cov_matrix_val = cov_matrix[val_indices][:, val_indices]\n            pred_val = pred[i][val_indices]\n        else:\n            pred[i] = pred[i].squeeze()\n\n        losses = []\n\n        model.train()\n\n        while counter &lt; max_epochs:\n            if max_epochs &lt; training_length:\n                break\n\n            training_length += 1\n            optimizer.zero_grad()\n            y_pred = model(x_alphas)\n\n            y_preds = torch.matmul(fk_tables, y_pred.flatten()) * binwidths.flatten()\n\n            y_preds = y_preds.squeeze()\n\n            y_int_mu = model(x_int)\n            y_int_mub = y_int_mu\n\n            if validation_split != 0.0:\n                y_train = y_preds[train_indices]\n                y_val = y_preds[val_indices]\n\n                loss_val = criterion(\n                    y_val,\n                    pred_val,\n                    cov_matrix_val,\n                    y_pred,\n                    y_int_mu,\n                    y_int_mub,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n\n                loss = criterion(\n                    y_train,\n                    pred_train,\n                    cov_matrix_train,\n                    y_pred,\n                    y_int_mu,\n                    y_int_mub,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n            else:\n                loss = criterion(\n                    y_preds,\n                    pred[i],\n                    cov_matrix,\n                    y_pred,\n                    y_int_mu,\n                    y_int_mub,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n\n            loss.backward()\n\n            if training_length % 500 == 0:\n                chi_squares.append(loss.detach().numpy())\n                if validation_split != 0.0:\n                    val_losses.append(loss_val)\n\n            losses.append(loss.detach().numpy())\n            optimizer.step()\n\n            if loss &lt; best_loss:\n                best_loss = loss\n                counter = 0\n            else:\n                counter += 1\n\n        if loss &lt; max_chi_sq:\n            f_nu = model(x_vals).detach().numpy().flatten()\n\n            chi_square_for_postfit.append(loss.detach().numpy())\n\n            preproc_pdfs.append(model.preproces(x_vals).detach().numpy().flatten())\n            nn_pdf = model.neuralnet(x_vals)[:, 0].detach().numpy().flatten()\n            nn_pdfs.append(nn_pdf)\n\n            N_event_pred.append(y_preds.detach().numpy())\n\n            neutrino_pdfs.append(f_nu)\n\n    return (\n        chi_squares,\n        N_event_pred,\n        neutrino_pdfs,\n        model,\n        chi_square_for_postfit,\n        train_indices,\n        val_indices,\n        training_length,\n        val_losses,\n    )\n</code></pre>"},{"location":"api/#perform_fit_nu_nubpy","title":"<code>perform_fit_nu_nub.py</code>","text":""},{"location":"api/#NN_fit.perform_fit_nu_nub.perform_fit","title":"<code>perform_fit(pred, num_reps, range_alpha, range_beta, range_gamma, lr, wd, patience, x_alphas, fk_tables_mu, fk_tables_mub, binwidths_mu, binwidths_mub, cov_matrix, extended_loss, activation_function, num_input_layers, num_output_layers, hidden_layers, x_vals, preproc, validation_split, max_epochs, max_chi_sq, fit_faser_data, lag_mult_pos, lag_mult_int, x_int)</code>","text":"<p>Performs repeated training of neural networks to fit pseudo-data predictions using a physics-constrained loss function and neural PDF parameterization.</p> <p>Each replica (<code>num_reps</code>) of the prediction is fitted using a randomly initialized neural network (based on PreprocessedMLP), and the corresponding predictions and losses are recorded. Optionally uses a validation split.</p>"},{"location":"api/#NN_fit.perform_fit_nu_nub.perform_fit--parameters","title":"Parameters","text":"<p>pred : list of np.ndarray     List of length <code>num_reps</code>, each containing the pseudo-data event predictions. num_reps : int     Number of replicas (i.e., independent fits with random initialization). range_alpha : float     Upper bound for uniform sampling of <code>alpha</code> preprocessing parameter. range_beta : float     Upper bound for uniform sampling of <code>beta</code> preprocessing parameter. range_gamma : float     Upper bound for uniform sampling of <code>gamma</code> preprocessing parameter. lr : float     Learning rate for the Adam optimizer. wd : float     Weight decay (L2 regularization) for the optimizer. patience : int     Early stopping patience (currently unused but declared). x_alphas : torch.Tensor     Input x-values used to evaluate PDF predictions for data loss. fk_tables : torch.Tensor     FastKernel tables to convert PDFs into observable space. binwidths : torch.Tensor     Widths of each bin used in rebinning the predictions. cov_matrix : np.ndarray     Covariance matrix used for weighted loss calculation. extended_loss : bool     If True, uses extended loss with constraints (e.g., normalization, positivity). activation_function : str     Activation function used in the neural network (e.g., 'relu', 'tanh'). num_input_layers : int     Number of input layers before hidden layers. num_output_layers : int     Number of output layers after hidden layers. hidden_layers : list of int     Number of neurons in each hidden layer. x_vals : np.ndarray     x-values used to store final fitted PDFs. preproc : str     Type of preprocessing function (e.g., 'powerlaw', 'exp') applied to PDFs. validation_split : float     Fraction of data used for validation (between 0 and 1). max_epochs : int     Maximum number of training epochs. max_chi_sq : float     Maximum allowed chi-squared for a fit to be accepted. lag_mult_pos : float     Lagrange multiplier for positivity constraint. lag_mult_int : float     Lagrange multiplier for integral constraint. x_int : np.ndarray     x-values used for computing integral constraints.</p>"},{"location":"api/#NN_fit.perform_fit_nu_nub.perform_fit--returns","title":"Returns","text":"<p>chi_squares : list of float     Training loss (chi-squared) values saved periodically during training. N_event_pred : list of np.ndarray     Predicted event yields after applying the FastKernel convolution. neutrino_pdfs : list of np.ndarray     Final predicted PDFs (postprocessed) from successful fits. model : PreprocessedMLP     Final trained model (from last accepted fit). chi_square_for_postfit : list of float     Final chi-squared values for each accepted fit (for post-fit evaluation). train_indices : np.ndarray     Indices used for training set in the last run (if validation was used). val_indices : np.ndarray     Indices used for validation set in the last run (if validation was used). training_length : int     Number of training steps run in the final (last) model.</p>"},{"location":"api/#NN_fit.perform_fit_nu_nub.perform_fit--notes","title":"Notes","text":"<ul> <li>Only models with <code>loss &lt; max_chi_sq</code> are retained in the final output.</li> <li>The PDFs are preprocessed using a parameterized function with random \u03b1, \u03b2, \u03b3 values.</li> <li>Assumes the model class <code>PreprocessedMLP</code> and loss class <code>CustomLoss</code> are defined externally.</li> <li>Model and predictions use PyTorch; inputs must be tensors where appropriate.</li> <li>Currently no explicit early stopping logic is implemented (but patience is reserved).</li> </ul> Source code in <code>NN_fit/perform_fit_nu_nub.py</code> <pre><code>def perform_fit(\n    pred: List[np.ndarray],\n    num_reps: int,\n    range_alpha: float,\n    range_beta: float,\n    range_gamma: float,\n    lr: float,\n    wd: float,\n    patience: int,\n    x_alphas: torch.Tensor,\n    fk_tables_mu: torch.Tensor,\n    fk_tables_mub: torch.Tensor,\n    binwidths_mu: torch.Tensor,\n    binwidths_mub: torch.Tensor,\n    cov_matrix: np.ndarray,\n    extended_loss: bool,\n    activation_function: str,\n    num_input_layers: int,\n    num_output_layers: int,\n    hidden_layers: List[int],\n    x_vals: np.ndarray,\n    preproc: str,\n    validation_split: float,\n    max_epochs: int,\n    max_chi_sq: float,\n    fit_faser_data: bool,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n    x_int: np.ndarray,\n) -&gt; Tuple[\n    List[float],  # chi_squares\n    List[np.ndarray],  # N_event_pred_mu\n    List[np.ndarray],  # N_event_pred_mub\n    List[np.ndarray],  # neutrino_pdfs_mu\n    List[np.ndarray],  # neutrino_pdfs_mub\n    PreprocessedMLP,  # model (last accepted fit)\n    List[float],  # chi_square_for_postfit\n    np.ndarray,  # train_indices\n    np.ndarray,  # val_indices\n    int,  # training_length\n]:\n    \"\"\"\n    Performs repeated training of neural networks to fit pseudo-data predictions using a\n    physics-constrained loss function and neural PDF parameterization.\n\n    Each replica (`num_reps`) of the prediction is fitted using a randomly initialized\n    neural network (based on PreprocessedMLP), and the corresponding predictions and losses\n    are recorded. Optionally uses a validation split.\n\n    Parameters\n    ----------\n    pred : list of np.ndarray\n        List of length `num_reps`, each containing the pseudo-data event predictions.\n    num_reps : int\n        Number of replicas (i.e., independent fits with random initialization).\n    range_alpha : float\n        Upper bound for uniform sampling of `alpha` preprocessing parameter.\n    range_beta : float\n        Upper bound for uniform sampling of `beta` preprocessing parameter.\n    range_gamma : float\n        Upper bound for uniform sampling of `gamma` preprocessing parameter.\n    lr : float\n        Learning rate for the Adam optimizer.\n    wd : float\n        Weight decay (L2 regularization) for the optimizer.\n    patience : int\n        Early stopping patience (currently unused but declared).\n    x_alphas : torch.Tensor\n        Input x-values used to evaluate PDF predictions for data loss.\n    fk_tables : torch.Tensor\n        FastKernel tables to convert PDFs into observable space.\n    binwidths : torch.Tensor\n        Widths of each bin used in rebinning the predictions.\n    cov_matrix : np.ndarray\n        Covariance matrix used for weighted loss calculation.\n    extended_loss : bool\n        If True, uses extended loss with constraints (e.g., normalization, positivity).\n    activation_function : str\n        Activation function used in the neural network (e.g., 'relu', 'tanh').\n    num_input_layers : int\n        Number of input layers before hidden layers.\n    num_output_layers : int\n        Number of output layers after hidden layers.\n    hidden_layers : list of int\n        Number of neurons in each hidden layer.\n    x_vals : np.ndarray\n        x-values used to store final fitted PDFs.\n    preproc : str\n        Type of preprocessing function (e.g., 'powerlaw', 'exp') applied to PDFs.\n    validation_split : float\n        Fraction of data used for validation (between 0 and 1).\n    max_epochs : int\n        Maximum number of training epochs.\n    max_chi_sq : float\n        Maximum allowed chi-squared for a fit to be accepted.\n    lag_mult_pos : float\n        Lagrange multiplier for positivity constraint.\n    lag_mult_int : float\n        Lagrange multiplier for integral constraint.\n    x_int : np.ndarray\n        x-values used for computing integral constraints.\n\n    Returns\n    -------\n    chi_squares : list of float\n        Training loss (chi-squared) values saved periodically during training.\n    N_event_pred : list of np.ndarray\n        Predicted event yields after applying the FastKernel convolution.\n    neutrino_pdfs : list of np.ndarray\n        Final predicted PDFs (postprocessed) from successful fits.\n    model : PreprocessedMLP\n        Final trained model (from last accepted fit).\n    chi_square_for_postfit : list of float\n        Final chi-squared values for each accepted fit (for post-fit evaluation).\n    train_indices : np.ndarray\n        Indices used for training set in the last run (if validation was used).\n    val_indices : np.ndarray\n        Indices used for validation set in the last run (if validation was used).\n    training_length : int\n        Number of training steps run in the final (last) model.\n\n    Notes\n    -----\n    - Only models with `loss &lt; max_chi_sq` are retained in the final output.\n    - The PDFs are preprocessed using a parameterized function with random \u03b1, \u03b2, \u03b3 values.\n    - Assumes the model class `PreprocessedMLP` and loss class `CustomLoss` are defined externally.\n    - Model and predictions use PyTorch; inputs must be tensors where appropriate.\n    - Currently no explicit early stopping logic is implemented (but patience is reserved).\n    \"\"\"\n    (\n        neutrino_pdfs_mu,\n        neutrino_pdfs_mub,\n        N_event_pred_mu,\n        N_event_pred_mub,\n        chi_squares,\n        preproc_pdfs,\n        nn_pdfs,\n        chi_square_for_postfit,\n        val_losses,\n    ) = (\n        [],\n        [],\n        [],\n        [],\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n    x_vals = torch.tensor(x_vals, dtype=torch.float32).view(-1, 1)\n    x_int = torch.tensor(x_int, dtype=torch.float32).view(-1, 1)\n    training_length = 0\n    for i in range(num_reps):\n        alpha, beta, gamma = (\n            np.random.rand() * range_alpha,\n            np.random.rand() * range_beta,\n            np.random.rand() * range_gamma,\n        )\n\n        model = PreprocessedMLP(\n            alpha,\n            beta,\n            gamma,\n            activation_function,\n            hidden_layers,\n            num_input_layers,\n            num_output_layers,\n            preproc=preproc,\n        )\n\n        criterion = CustomLoss(\n            extended_loss,\n            num_output_layers,\n        )\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\n        dataset_size = pred[i].shape[0]\n        indices = np.arange(dataset_size)\n        # np.random.seed(seed)\n        np.random.shuffle(indices)\n        val_size = int(dataset_size * validation_split)\n\n        train_indices, val_indices = indices[val_size:], indices[:val_size]\n\n        if validation_split != 0:\n            pred_train = pred[i][train_indices]\n            cov_matrix_train = cov_matrix[train_indices][:, train_indices]\n            cov_matrix_val = cov_matrix[val_indices][:, val_indices]\n            pred_val = pred[i][val_indices]\n        else:\n            pred[i] = pred[i].squeeze()\n\n        losses = []\n\n        model.train()\n        best_loss = 1e13\n        counter = 0\n\n        while counter &lt; patience:\n            if max_epochs &lt; training_length:\n                break\n            training_length += 1\n            optimizer.zero_grad()\n            y_pred = model(x_alphas)\n\n            y_pred_mu = (\n                torch.matmul(fk_tables_mu, y_pred[:, 0]) * binwidths_mu.flatten()\n            )\n\n            y_pred_mub = (\n                torch.matmul(fk_tables_mub, y_pred[:, 1]) * binwidths_mub.flatten()\n            )\n\n            y_pred_mu = y_pred_mu.squeeze()\n            y_pred_mub = y_pred_mub.squeeze()\n\n            if fit_faser_data:\n                y_pred_mu[-1] = y_pred_mu[-1] + y_pred_mub[-1]\n                y_pred_mub = y_pred_mub[:-1]\n\n                y_pred_mub = torch.flip(y_pred_mub, dims=[0])\n\n            y_preds = torch.hstack((y_pred_mu, y_pred_mub))\n            # y_preds = y_pred_mu + y_pred_mub  # torch hstack\n            # y_preds = y_pred_mu\n\n            y_int_mu = model(x_int)[:, 0]\n            y_int_mub = model(x_int)[:, 1]\n            y_pred = model(x_alphas)\n            if validation_split != 0.0:\n                y_train = y_preds[train_indices]\n                y_val = y_preds[val_indices]\n\n                loss_val = criterion(\n                    y_val,\n                    pred_val,\n                    cov_matrix_val,\n                    y_pred,\n                    y_int_mu,\n                    y_int_mub,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n\n                loss = criterion(\n                    y_train,\n                    pred_train,\n                    cov_matrix_train,\n                    y_pred,\n                    y_int_mu,\n                    y_int_mub,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n            else:\n                loss = criterion(\n                    y_preds,\n                    pred[i],\n                    cov_matrix,\n                    y_pred,\n                    y_int_mu,\n                    y_int_mub,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n\n            loss.backward()\n\n            if training_length % 500 == 0:\n                chi_squares.append(loss.detach().numpy())\n                if validation_split != 0.0:\n                    val_losses.append(loss_val)\n\n            losses.append(loss.detach().numpy())\n            optimizer.step()\n\n            if loss &lt; best_loss:\n                best_loss = loss\n                counter = 0\n            else:\n                counter += 1\n\n        if loss &lt; max_chi_sq:\n            f_nu_mub = model(x_vals)[:, 1].detach().numpy().flatten()\n            f_nu_mu = model(x_vals)[:, 0].detach().numpy().flatten()\n\n            chi_square_for_postfit.append(loss.detach().numpy())\n\n            preproc_pdfs.append(model.preproces(x_vals).detach().numpy().flatten())\n            nn_pdf = model.neuralnet(x_vals)[:, 0].detach().numpy().flatten()\n            nn_pdfs.append(nn_pdf)\n\n            N_event_pred_mu.append(y_pred_mu.detach().numpy())\n            N_event_pred_mub.append(y_pred_mub.detach().numpy())\n\n            neutrino_pdfs_mu.append(f_nu_mu)\n            neutrino_pdfs_mub.append(f_nu_mub)\n    return (\n        chi_squares,\n        N_event_pred_mu,\n        N_event_pred_mub,\n        neutrino_pdfs_mu,\n        neutrino_pdfs_mub,\n        model,\n        chi_square_for_postfit,\n        train_indices,\n        val_indices,\n        training_length,\n        val_losses,\n    )\n</code></pre>"},{"location":"api/#execute_fitpy","title":"<code>execute_fit.py</code>","text":""},{"location":"api/#execute_postfitpy","title":"<code>execute_postfit.py</code>","text":""},{"location":"api/#NN_fit.execute_postfit.postfit_execution","title":"<code>postfit_execution(postfit_criteria, validation_split, data, cov_matrix, num_output_layers, chi_square_for_postfit, neutrino_pdfs_mu, neutrino_pdfs_mub, neutrino_pdfs, postfit_measures, train_indices, val_indices, level1, N_event_pred, pred, dir_for_data, filename_postfit, diff_lev_1, fit_level, x_alphas, pdf, pdf_set, particle_id_nu, particle_id_nub, lr, wd, max_epochs, patience, chi_squares, neutrino_pdf_fit_name_lhapdf, x_vals, produce_plot, training_lengths, stat_error, sys_error, low_bin, high_bin, N_event_pred_nu, N_event_pred_nub, low_bin_mu, high_bin_mu, low_bin_mub, high_bin_mub, val_losses)</code>","text":"<p>Execute post-fit processing after a PDF (Parton Distribution Function) neural fit.</p> <p>This function performs various operations following a neural network-based PDF fit: - Applies post-fit criteria to PDFs and predictions. - Calculates post-fit measures (e.g., delta chi-squared, phi, bias-to-variance). - Logs results and configuration to file. - Writes PDF replicas and uncertainties to LHAPDF-compatible grid files. - Optionally generates plots of the results.</p> <p>Parameters:</p> Name Type Description Default <code>postfit_criteria</code> <code>bool</code> <p>Whether to apply post-fit criteria to the predictions and PDFs.</p> required <code>validation_split</code> <code>float</code> <p>Fraction of data used for validation. If 0, no validation is used.</p> required <code>data</code> <code>ndarray</code> <p>Measured experimental data.</p> required <code>cov_matrix</code> <code>ndarray</code> <p>Covariance matrix of the data.</p> required <code>num_output_layers</code> <code>int</code> <p>Number of neural network outputs (1 for single PDF, 2 for neutrino/antineutrino PDFs).</p> required <code>chi_square_for_postfit</code> <code>ndarray</code> <p>Chi-square values for the post-fit predictions.</p> required <code>neutrino_pdfs_mu</code> <code>ndarray</code> <p>Neutrino PDFs for muon neutrino (only if <code>num_output_layers==2</code>).</p> required <code>neutrino_pdfs_mub</code> <code>ndarray</code> <p>Neutrino PDFs for anti-muon neutrino (only if <code>num_output_layers==2</code>).</p> required <code>neutrino_pdfs</code> <code>ndarray</code> <p>Neutrino PDFs for the single-output model.</p> required <code>postfit_measures</code> <code>bool</code> <p>Whether to compute post-fit performance metrics.</p> required <code>train_indices</code> <code>ndarray</code> <p>Training sample indices.</p> required <code>val_indices</code> <code>ndarray</code> <p>Validation sample indices.</p> required <code>level1</code> <code>ndarray</code> <p>Level-1 shifts or corrections.</p> required <code>N_event_pred</code> <code>ndarray</code> <p>Predicted number of events from the model.</p> required <code>pred</code> <code>ndarray</code> <p>Model predictions.</p> required <code>dir_for_data</code> <code>str</code> <p>Directory to save results and intermediate files.</p> required <code>filename_postfit</code> <code>str</code> <p>Name of the post-fit report file.</p> required <code>diff_lev_1</code> <code>str / int</code> <p>Identifier for the level-1 difference, used in LHAPDF naming.</p> required <code>fit_level</code> <code>int</code> <p>Level of fit used; affects which postfit measures are computed.</p> required <code>x_alphas</code> <code>Tensor</code> <p>Neural network output alphas (PDF coefficients).</p> required <code>pdf</code> <code>object</code> <p>Reference PDF used during fitting.</p> required <code>pdf_set</code> <code>str</code> <p>Name of the PDF set used as baseline.</p> required <code>particle_id_nu</code> <code>int</code> <p>PDG ID for the neutrino used.</p> required <code>particle_id_nub</code> <code>int</code> <p>PDG ID for the anti-neutrino used.</p> required <code>lr</code> <code>float</code> <p>Learning rate used in the training.</p> required <code>wd</code> <code>float</code> <p>Weight decay used during training.</p> required <code>max_epochs</code> <code>int</code> <p>Maximum number of training epochs.</p> required <code>patience</code> <code>int</code> <p>Early stopping patience.</p> required <code>chi_squares</code> <code>ndarray</code> <p>Chi-square values for the fit.</p> required <code>neutrino_pdf_fit_name_lhapdf</code> <code>str</code> <p>Name to use for the LHAPDF set.</p> required <code>x_vals</code> <code>ndarray</code> <p>X-values (momentum fraction) for PDF grids.</p> required <code>produce_plot</code> <code>bool</code> <p>Whether to produce summary plots of fit results.</p> required <code>training_lengths</code> <code>ndarray</code> <p>Number of epochs run for each replica.</p> required <code>stat_error</code> <code>ndarray</code> <p>Statistical error on the data.</p> required <code>sys_error</code> <code>ndarray</code> <p>Systematic error on the data.</p> required <code>low_bin</code> <code>int</code> <p>Lower bound for binning (single PDF).</p> required <code>high_bin</code> <code>int</code> <p>Upper bound for binning (single PDF).</p> required <code>N_event_pred_nu</code> <code>ndarray</code> <p>Event predictions for muon neutrino (two-output case).</p> required <code>N_event_pred_nub</code> <code>ndarray</code> <p>Event predictions for anti-muon neutrino (two-output case).</p> required <code>low_bin_mu</code> <code>int</code> <p>Lower bin index for muon neutrino.</p> required <code>high_bin_mu</code> <code>int</code> <p>Upper bin index for muon neutrino.</p> required <code>low_bin_mub</code> <code>int</code> <p>Lower bin index for anti-muon neutrino.</p> required <code>high_bin_mub</code> <code>int</code> <p>Upper bin index for anti-muon neutrino.</p> required Outputs <ul> <li>Writes various <code>.txt</code> files with statistical and fit information.</li> <li>Creates LHAPDF-compatible grid files with the fitted PDFs.</li> <li>Optionally generates plots summarizing the fit quality and predictions.</li> </ul> Notes <ul> <li>Assumes presence of <code>Postfit</code>, <code>Measures</code>, and LHAPDF writing utilities.</li> <li>Assumes external plotting utilities (<code>plot_comb_pdf_cl</code>, <code>plot_nu_nub_cl</code>) are available.</li> <li>Handles both single-output (combined neutrino) and two-output (neutrino/antineutrino) models.</li> </ul> Source code in <code>NN_fit/execute_postfit.py</code> <pre><code>def postfit_execution(\n    postfit_criteria: bool,\n    validation_split: float,\n    data: np.ndarray,\n    cov_matrix: np.ndarray,\n    num_output_layers: int,\n    chi_square_for_postfit: np.ndarray,\n    neutrino_pdfs_mu: Optional[np.ndarray],\n    neutrino_pdfs_mub: Optional[np.ndarray],\n    neutrino_pdfs: Optional[np.ndarray],\n    postfit_measures: bool,\n    train_indices: np.ndarray,\n    val_indices: np.ndarray,\n    level1: np.ndarray,\n    N_event_pred: np.ndarray,\n    pred: np.ndarray,\n    dir_for_data: str,\n    filename_postfit: str,\n    diff_lev_1: Union[str, int],\n    fit_level: int,\n    x_alphas: torch.Tensor,\n    pdf: object,\n    pdf_set: str,\n    particle_id_nu: int,\n    particle_id_nub: int,\n    lr: float,\n    wd: float,\n    max_epochs: int,\n    patience: int,\n    chi_squares: np.ndarray,\n    neutrino_pdf_fit_name_lhapdf: str,\n    x_vals: np.ndarray,\n    produce_plot: bool,\n    training_lengths: np.ndarray,\n    stat_error: np.ndarray,\n    sys_error: np.ndarray,\n    low_bin: int,\n    high_bin: int,\n    N_event_pred_nu: Optional[np.ndarray],\n    N_event_pred_nub: Optional[np.ndarray],\n    low_bin_mu: Optional[int],\n    high_bin_mu: Optional[int],\n    low_bin_mub: Optional[int],\n    high_bin_mub: Optional[int],\n    val_losses: list,\n):\n    \"\"\"\n    Execute post-fit processing after a PDF (Parton Distribution Function) neural fit.\n\n    This function performs various operations following a neural network-based PDF fit:\n    - Applies post-fit criteria to PDFs and predictions.\n    - Calculates post-fit measures (e.g., delta chi-squared, phi, bias-to-variance).\n    - Logs results and configuration to file.\n    - Writes PDF replicas and uncertainties to LHAPDF-compatible grid files.\n    - Optionally generates plots of the results.\n\n    Parameters:\n        postfit_criteria (bool): Whether to apply post-fit criteria to the predictions and PDFs.\n        validation_split (float): Fraction of data used for validation. If 0, no validation is used.\n        data (np.ndarray): Measured experimental data.\n        cov_matrix (np.ndarray): Covariance matrix of the data.\n        num_output_layers (int): Number of neural network outputs (1 for single PDF, 2 for neutrino/antineutrino PDFs).\n        chi_square_for_postfit (np.ndarray): Chi-square values for the post-fit predictions.\n        neutrino_pdfs_mu (np.ndarray): Neutrino PDFs for muon neutrino (only if `num_output_layers==2`).\n        neutrino_pdfs_mub (np.ndarray): Neutrino PDFs for anti-muon neutrino (only if `num_output_layers==2`).\n        neutrino_pdfs (np.ndarray): Neutrino PDFs for the single-output model.\n        postfit_measures (bool): Whether to compute post-fit performance metrics.\n        train_indices (np.ndarray): Training sample indices.\n        val_indices (np.ndarray): Validation sample indices.\n        level1 (np.ndarray): Level-1 shifts or corrections.\n        N_event_pred (np.ndarray): Predicted number of events from the model.\n        pred (np.ndarray): Model predictions.\n        dir_for_data (str): Directory to save results and intermediate files.\n        filename_postfit (str): Name of the post-fit report file.\n        diff_lev_1 (str/int): Identifier for the level-1 difference, used in LHAPDF naming.\n        fit_level (int): Level of fit used; affects which postfit measures are computed.\n        x_alphas (torch.Tensor): Neural network output alphas (PDF coefficients).\n        pdf (object): Reference PDF used during fitting.\n        pdf_set (str): Name of the PDF set used as baseline.\n        particle_id_nu (int): PDG ID for the neutrino used.\n        particle_id_nub (int): PDG ID for the anti-neutrino used.\n        lr (float): Learning rate used in the training.\n        wd (float): Weight decay used during training.\n        max_epochs (int): Maximum number of training epochs.\n        patience (int): Early stopping patience.\n        chi_squares (np.ndarray): Chi-square values for the fit.\n        neutrino_pdf_fit_name_lhapdf (str): Name to use for the LHAPDF set.\n        x_vals (np.ndarray): X-values (momentum fraction) for PDF grids.\n        produce_plot (bool): Whether to produce summary plots of fit results.\n        training_lengths (np.ndarray): Number of epochs run for each replica.\n        stat_error (np.ndarray): Statistical error on the data.\n        sys_error (np.ndarray): Systematic error on the data.\n        low_bin (int): Lower bound for binning (single PDF).\n        high_bin (int): Upper bound for binning (single PDF).\n        N_event_pred_nu (np.ndarray): Event predictions for muon neutrino (two-output case).\n        N_event_pred_nub (np.ndarray): Event predictions for anti-muon neutrino (two-output case).\n        low_bin_mu (int): Lower bin index for muon neutrino.\n        high_bin_mu (int): Upper bin index for muon neutrino.\n        low_bin_mub (int): Lower bin index for anti-muon neutrino.\n        high_bin_mub (int): Upper bin index for anti-muon neutrino.\n\n    Outputs:\n        - Writes various `.txt` files with statistical and fit information.\n        - Creates LHAPDF-compatible grid files with the fitted PDFs.\n        - Optionally generates plots summarizing the fit quality and predictions.\n\n    Notes:\n        - Assumes presence of `Postfit`, `Measures`, and LHAPDF writing utilities.\n        - Assumes external plotting utilities (`plot_comb_pdf_cl`, `plot_nu_nub_cl`) are available.\n        - Handles both single-output (combined neutrino) and two-output (neutrino/antineutrino) models.\n    \"\"\"\n    if postfit_criteria:\n        train_indices = train_indices.reshape(1, -1)\n        val_indices = val_indices.reshape(1, -1)\n\n        level1 = level1[0]\n\n        if validation_split != 0.0:\n            train_indices = train_indices[0]\n            val_indices = val_indices[0]\n            train_indices = train_indices.astype(int)\n            val_indices = val_indices.astype(int)\n\n            N_event_pred_train = N_event_pred[:, train_indices]\n            pred_train = pred[:, train_indices]\n\n            N_event_pred_val = N_event_pred[:, val_indices]\n            data_val = data[val_indices]\n            pred_val = pred[:, val_indices]\n\n            level1_val = level1[val_indices]\n\n            val_indices = torch.tensor(val_indices)\n\n            cov_matrix_val = cov_matrix[val_indices][:, val_indices]\n\n        if num_output_layers == 1:\n\n            def compute_postfit_criteria(neutrino_pdfs, N_event_pred, pred):\n                closure_fit = Postfit()\n                neutrino_pdfs, N_event_pred, pred = closure_fit.apply_postfit_criteria(\n                    chi_square_for_postfit, N_event_pred, neutrino_pdfs, pred\n                )\n                return (neutrino_pdfs, N_event_pred, pred)\n\n            if postfit_criteria and validation_split != 0.0:\n                neutrino_pdfs, N_event_pred_train, pred_train = (\n                    compute_postfit_criteria(\n                        neutrino_pdfs, N_event_pred_train, pred_train\n                    )\n                )\n            if postfit_criteria and validation_split == 0:\n                neutrino_pdfs, N_event_pred, pred = compute_postfit_criteria(\n                    neutrino_pdfs, N_event_pred, pred\n                )\n        if num_output_layers == 2:\n\n            def compute_postfit_criteria(\n                neutrino_pdfs_mu, neutrino_pdfs_mub, N_event_pred, pred\n            ):\n                # if postfit_criteria:\n                closure_fit = Postfit()\n                neutrino_pdfs_mu, _, _ = closure_fit.apply_postfit_criteria(\n                    chi_square_for_postfit, N_event_pred, neutrino_pdfs_mu, pred\n                )\n                neutrino_pdfs_mub, N_event_pred, pred = (\n                    closure_fit.apply_postfit_criteria(\n                        chi_square_for_postfit, N_event_pred, neutrino_pdfs_mub, pred\n                    )\n                )\n\n            if postfit_criteria and validation_split != 0.0:\n                compute_postfit_criteria(\n                    neutrino_pdfs_mu, neutrino_pdfs_mub, N_event_pred_train, pred_train\n                )\n            if postfit_criteria and validation_split == 0:\n                compute_postfit_criteria(\n                    neutrino_pdfs_mu, neutrino_pdfs_mub, N_event_pred, pred\n                )\n\n    if postfit_measures:\n        with open(f\"{dir_for_data}/{filename_postfit}\", \"w\") as file:\n            file.write(f\"level 1 shift {diff_lev_1}:\\n\")\n            file.write(\"postfit report faser sim fit:\\n\")\n            file.write(\"100 replicas:\\n\")\n\n        def compute_postfit_measures(cov_matrix, N_event_pred, data, level1, pred):\n            compute_postfit = Measures(cov_matrix, pdf, N_event_pred)\n            if fit_level != 0:\n                delta_chi = compute_postfit.compute_delta_chi(\n                    data,\n                    N_event_pred,\n                    level1,\n                    x_alphas.detach().numpy().squeeze(),\n                )\n\n                with open(f\"{dir_for_data}/{filename_postfit}\", \"a\") as file:\n                    file.write(f\"delta chi^2 = {delta_chi}:\\n\")\n\n                if num_output_layers == 1:\n                    accuracy = compute_postfit.compute_accuracy(\n                        x_alphas.detach().numpy().flatten(),\n                        neutrino_pdfs,\n                        pdf,\n                        1,\n                        pdf_set,\n                        particle_id_nu,\n                    )\n\n                    with open(f\"{dir_for_data}/{filename_postfit}\", \"a\") as file:\n                        file.write(f\"accuracy = {accuracy}:\\n\")\n                if num_output_layers == 2:\n                    accuracy_nu = compute_postfit.compute_accuracy(\n                        x_alphas.detach().numpy().flatten(),\n                        neutrino_pdfs_mu,\n                        pdf,\n                        1,\n                        pdf_set,\n                        particle_id_nu,\n                    )\n\n                    accuracy_nub = compute_postfit.compute_accuracy(\n                        x_alphas.detach().numpy().flatten(),\n                        neutrino_pdfs_mub,\n                        pdf,\n                        1,\n                        pdf_set,\n                        particle_id_nub,\n                    )\n\n                    with open(f\"{dir_for_data}/{filename_postfit}\", \"a\") as file:\n                        file.write(f\"accuracy nu = {accuracy_nu}:\\n\")\n                        file.write(f\"accuracy nub = {accuracy_nub}:\\n\")\n\n            phi = compute_postfit.compute_phi(data, chi_square_for_postfit)\n\n            with open(f\"{dir_for_data}/{filename_postfit}\", \"a\") as file:\n                file.write(f\"phi = {phi}:\\n\")\n\n            if fit_level == 2:\n                bias_to_var = compute_postfit.compute_bias_to_variance(\n                    data, pred, N_event_pred, len(N_event_pred)\n                )\n\n                with open(f\"{dir_for_data}/{filename_postfit}\", \"a\") as file:\n                    file.write(f\"bias_to_var = {bias_to_var}:\\n\")\n\n        if postfit_measures and validation_split != 0.0:\n            compute_postfit_measures(\n                cov_matrix_val, N_event_pred_val, data_val, level1_val, pred_val\n            )\n        if postfit_measures and validation_split == 0.0:\n            compute_postfit_measures(cov_matrix, N_event_pred, data, level1, pred)\n\n        with open(f\"{dir_for_data}/{filename_postfit}\", \"a\") as file:\n            file.write(f\"mean chi^2 = {np.mean(chi_square_for_postfit)}:\\n\")\n            file.write(f\"average training length = {np.mean(training_lengths)}:\\n\")\n            file.write(\"settings used:\\n\")\n            file.write(f\"learning rate = {lr}:\\n\")\n            file.write(f\"weigth decay = {wd}:\\n\")\n            file.write(f\"max training lenght = {max_epochs}:\\n\")\n            file.write(f\"patience = {patience}:\\n\")\n\n    with open(f\"{dir_for_data}/chi_square.txt\", \"w\") as f:\n        np.savetxt(f, chi_squares, delimiter=\",\")\n\n    with open(f\"{dir_for_data}/chi_squares_for_postfit.txt\", \"w\") as f:\n        np.savetxt(f, chi_square_for_postfit, delimiter=\",\")\n\n    with open(f\"{dir_for_data}/events.txt\", \"w\") as f:\n        np.savetxt(f, N_event_pred, delimiter=\",\")\n\n    os.makedirs(\n        f\"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/{neutrino_pdf_fit_name_lhapdf}_{diff_lev_1}\",\n        exist_ok=True,\n    )\n    template_path = \"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/template_.info\"\n    path = f\"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/{neutrino_pdf_fit_name_lhapdf}_{diff_lev_1}/{neutrino_pdf_fit_name_lhapdf}.info\"\n    set_index = int(np.random.rand() * 1e7)\n    pdf_dict_central = {}\n    pdf_dict_error = {}\n\n    if num_output_layers == 1:\n        customize_info_file(template_path, path, set_index, f\"{particle_id_nu}\", 2)\n        mean_pdf = np.mean(neutrino_pdfs, axis=0)\n        std_pdf = np.std(neutrino_pdfs, axis=0)\n        path = f\"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/{neutrino_pdf_fit_name_lhapdf}_{diff_lev_1}/{neutrino_pdf_fit_name_lhapdf}_0000.dat\"\n        pdf_dict_error[12] = mean_pdf\n        pdf_dict_central[12] = std_pdf\n        write_lhapdf_grid(x_vals, pdf_dict_central, path)\n        path = f\"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/{neutrino_pdf_fit_name_lhapdf}_{diff_lev_1}/{neutrino_pdf_fit_name_lhapdf}_0001.dat\"\n        write_lhapdf_grid(x_vals, pdf_dict_error, path)\n    if num_output_layers == 2:\n        customize_info_file(\n            template_path, path, set_index, f\"{particle_id_nu}, {particle_id_nub}\", 2\n        )\n        mean_pdf_nu = np.mean(neutrino_pdfs_mu, axis=0)\n        mean_pdf_nub = np.mean(neutrino_pdfs_mub, axis=0)\n        std_pdf_nu = np.std(neutrino_pdfs_mu, axis=0)\n        std_pdf_nub = np.std(neutrino_pdfs_mub, axis=0)\n        path = f\"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/{neutrino_pdf_fit_name_lhapdf}_{diff_lev_1}/{neutrino_pdf_fit_name_lhapdf}_0000.dat\"\n\n        pdf_dict_error[14] = mean_pdf_nu\n        pdf_dict_error[-14] = mean_pdf_nub\n        pdf_dict_central[14] = std_pdf_nu\n        pdf_dict_central[-14] = std_pdf_nub\n        write_lhapdf_grid(x_vals, pdf_dict_central, path)\n\n        path = f\"/opt/anaconda3/envs/test_lhapdf/share/LHAPDF/{neutrino_pdf_fit_name_lhapdf}_{diff_lev_1}/{neutrino_pdf_fit_name_lhapdf}_0001.dat\"\n        write_lhapdf_grid(x_vals, pdf_dict_error, path)\n\n    if len(val_losses) &gt; 0:\n        val_losses = np.array(val_losses)\n        with open(f\"{dir_for_data}/val_losses.txt\", \"w\") as f:\n            np.savetxt(f, val_losses, delimiter=\",\")\n\n    if chi_square_for_postfit.size != 0:\n        with open(f\"{dir_for_data}/pred.txt\", \"w\") as f:\n            np.savetxt(f, pred, delimiter=\",\")\n\n        with open(f\"{dir_for_data}/train_indices.txt\", \"w\") as f:\n            np.savetxt(f, train_indices, delimiter=\",\")\n        with open(f\"{dir_for_data}/val_indices.txt\", \"w\") as f:\n            np.savetxt(f, val_indices, delimiter=\",\")\n\n        with open(f\"{dir_for_data}/training_lengths.txt\", \"w\") as f:\n            np.savetxt(f, training_lengths, delimiter=\",\")\n\n    if produce_plot:\n        if num_output_layers == 1:\n            from plot_comb_pdf_cl import plot\n\n            sig_tot = np.sqrt(stat_error**2 + sys_error**2)\n            plot(\n                x_vals,\n                neutrino_pdfs,\n                data,\n                N_event_pred,\n                sig_tot,\n                particle_id_nu,\n                low_bin,\n                high_bin,\n                pdf,\n                pdf_set,\n                dir_for_data,\n            )\n        if num_output_layers == 2:\n            from plot_nu_nub_cl import plot\n\n            sig_tot = np.sqrt(stat_error**2 + sys_error**2)\n            plot(\n                x_vals,\n                neutrino_pdfs_mu,\n                neutrino_pdfs_mub,\n                data,\n                N_event_pred_nu,\n                N_event_pred_nub,\n                sig_tot,\n                particle_id_nu,\n                low_bin_mu,\n                high_bin_mu,\n                low_bin_mub,\n                high_bin_mub,\n                pdf,\n                pdf_set,\n                dir_for_data,\n            )\n</code></pre>"},{"location":"api/#hyperparameter-optimization","title":"\ud83c\udfaf Hyperparameter optimization","text":""},{"location":"api/#perform_hyperoptpy","title":"<code>perform_hyperopt.py</code>","text":""},{"location":"api/#hyperopt_combpy","title":"<code>hyperopt_comb.py</code>","text":""},{"location":"api/#NN_fit.hyperopt_comb.perform_fit","title":"<code>perform_fit(pred, range_alpha, range_beta, range_gamma, lr, wd, patience, x_alphas, fk_tables, binwidths, cov_matrix, extended_loss, activation_function, num_input_layers, num_output_layers, hidden_layers, x_vals, preproc, max_epochs, lag_mult_pos, lag_mult_int, x_int, num_folds)</code>","text":"<p>Trains a neural network model to fit pseudo-data for electron neutrino event predictions using K-fold cross-validation and physics-informed constraints.</p> <p>This function fits a parameterized neural network (PreprocessedMLP) using a custom loss function that incorporates statistical and physical constraints such as positivity and normalization. K-fold cross-validation is used to evaluate the model's generalization performance across different data splits. Random initialization of the preprocessing parameters (alpha, beta, gamma) enables exploration of a hyperparameter space.</p>"},{"location":"api/#NN_fit.hyperopt_comb.perform_fit--parameters","title":"Parameters","text":"<p>pred : List[np.ndarray]     List containing prediction arrays (pseudo-data) for electron neutrino event counts. range_alpha : float     Maximum value for randomly sampling the alpha preprocessing parameter. range_beta : float     Maximum value for randomly sampling the beta preprocessing parameter. range_gamma : float     Maximum value for randomly sampling the gamma preprocessing parameter. lr : float     Learning rate for the Adam optimizer. wd : float     Weight decay (L2 regularization) used during optimization. patience : int     Early stopping patience threshold (number of epochs without improvement before stopping). x_alphas : torch.Tensor     Input tensor used to evaluate the model's predicted PDFs. fk_tables : torch.Tensor     Forward-folding kernel that maps PDF space to observable event space. binwidths : torch.Tensor     Bin widths used to scale the convolved predictions. cov_matrix : np.ndarray     Covariance matrix of the pseudo-data, used for uncertainty-aware loss computation. extended_loss : bool     Whether to include extended physics constraints (e.g., positivity, integrals) in the loss. activation_function : str     Name of the activation function to be used in the MLP (e.g., 'relu', 'tanh'). num_input_layers : int     Number of input neurons to the network (typically 1 for univariate PDFs). num_output_layers : int     Number of output neurons (typically 1 for electron neutrinos). hidden_layers : List[int]     List of hidden layer sizes (e.g., [50, 50] for a 2-layer MLP with 50 neurons each). x_vals : np.ndarray     Input values over which the final PDF predictions will be evaluated. preproc : str     Type of preprocessing function used on the PDFs (e.g., 'log', 'powerlaw'). max_epochs : int     Maximum number of training epochs per fold. lag_mult_pos : float     Lagrange multiplier for the positivity constraint in the loss. lag_mult_int : float     Lagrange multiplier for the integral (normalization) constraint in the loss. x_int : np.ndarray     Input values used for evaluating the integral constraints on the PDF.</p>"},{"location":"api/#NN_fit.hyperopt_comb.perform_fit--returns","title":"Returns","text":"<p>chi_squares : List[float]     History of chi-squared values during training (saved periodically). N_event_pred : List[np.ndarray]     Placeholder for predicted event counts (not currently populated in this version). neutrino_pdfs : List[np.ndarray]     Placeholder for final PDF outputs (not currently populated in this version). model : PreprocessedMLP     Trained neural network model from the final fold. chi_square_for_postfit : List[float]     Final loss value (chi-squared) for each fold. train_indices : np.ndarray     Indices of the training samples used in the final fold. val_indices : np.ndarray     Indices of the validation samples used in the final fold. training_length : int     Number of epochs completed during the final fold training. num_folds: int     number of k-folds</p>"},{"location":"api/#NN_fit.hyperopt_comb.perform_fit--notes","title":"Notes","text":"<ul> <li>The function uses 3-fold cross-validation to evaluate generalization.</li> <li>Preprocessing parameters (alpha, beta, gamma) are randomized for each fold.</li> <li>This implementation supports only one prediction channel and assumes   symmetric treatment of integrals and positivity constraints.</li> <li><code>N_event_pred</code> and <code>neutrino_pdfs</code> are currently not returned meaningfully.</li> </ul> Source code in <code>NN_fit/hyperopt_comb.py</code> <pre><code>def perform_fit(\n    pred: List[np.ndarray],\n    range_alpha: float,\n    range_beta: float,\n    range_gamma: float,\n    lr: float,\n    wd: float,\n    patience: int,\n    x_alphas: torch.Tensor,\n    fk_tables: torch.Tensor,\n    binwidths: torch.Tensor,\n    cov_matrix: np.ndarray,\n    extended_loss: bool,\n    activation_function: str,\n    num_input_layers: int,\n    num_output_layers: int,\n    hidden_layers: List[int],\n    x_vals: np.ndarray,\n    preproc: str,\n    max_epochs: int,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n    x_int: np.ndarray,\n    num_folds: int,\n) -&gt; Tuple[\n    List[float],  # chi_squares\n    List[np.ndarray],  # N_event_pred\n    List[np.ndarray],  # neutrino_pdfs\n    PreprocessedMLP,  # model (last accepted fit)\n    List[float],  # chi_square_for_postfit\n    np.ndarray,  # train_indices\n    np.ndarray,  # val_indices\n    int,  # training_length\n]:\n    \"\"\"\n    Trains a neural network model to fit pseudo-data for electron neutrino event predictions\n    using K-fold cross-validation and physics-informed constraints.\n\n    This function fits a parameterized neural network (PreprocessedMLP) using a custom loss function\n    that incorporates statistical and physical constraints such as positivity and normalization.\n    K-fold cross-validation is used to evaluate the model's generalization performance across\n    different data splits. Random initialization of the preprocessing parameters (alpha, beta, gamma)\n    enables exploration of a hyperparameter space.\n\n    Parameters\n    ----------\n    pred : List[np.ndarray]\n        List containing prediction arrays (pseudo-data) for electron neutrino event counts.\n    range_alpha : float\n        Maximum value for randomly sampling the alpha preprocessing parameter.\n    range_beta : float\n        Maximum value for randomly sampling the beta preprocessing parameter.\n    range_gamma : float\n        Maximum value for randomly sampling the gamma preprocessing parameter.\n    lr : float\n        Learning rate for the Adam optimizer.\n    wd : float\n        Weight decay (L2 regularization) used during optimization.\n    patience : int\n        Early stopping patience threshold (number of epochs without improvement before stopping).\n    x_alphas : torch.Tensor\n        Input tensor used to evaluate the model's predicted PDFs.\n    fk_tables : torch.Tensor\n        Forward-folding kernel that maps PDF space to observable event space.\n    binwidths : torch.Tensor\n        Bin widths used to scale the convolved predictions.\n    cov_matrix : np.ndarray\n        Covariance matrix of the pseudo-data, used for uncertainty-aware loss computation.\n    extended_loss : bool\n        Whether to include extended physics constraints (e.g., positivity, integrals) in the loss.\n    activation_function : str\n        Name of the activation function to be used in the MLP (e.g., 'relu', 'tanh').\n    num_input_layers : int\n        Number of input neurons to the network (typically 1 for univariate PDFs).\n    num_output_layers : int\n        Number of output neurons (typically 1 for electron neutrinos).\n    hidden_layers : List[int]\n        List of hidden layer sizes (e.g., [50, 50] for a 2-layer MLP with 50 neurons each).\n    x_vals : np.ndarray\n        Input values over which the final PDF predictions will be evaluated.\n    preproc : str\n        Type of preprocessing function used on the PDFs (e.g., 'log', 'powerlaw').\n    max_epochs : int\n        Maximum number of training epochs per fold.\n    lag_mult_pos : float\n        Lagrange multiplier for the positivity constraint in the loss.\n    lag_mult_int : float\n        Lagrange multiplier for the integral (normalization) constraint in the loss.\n    x_int : np.ndarray\n        Input values used for evaluating the integral constraints on the PDF.\n\n    Returns\n    -------\n    chi_squares : List[float]\n        History of chi-squared values during training (saved periodically).\n    N_event_pred : List[np.ndarray]\n        Placeholder for predicted event counts (not currently populated in this version).\n    neutrino_pdfs : List[np.ndarray]\n        Placeholder for final PDF outputs (not currently populated in this version).\n    model : PreprocessedMLP\n        Trained neural network model from the final fold.\n    chi_square_for_postfit : List[float]\n        Final loss value (chi-squared) for each fold.\n    train_indices : np.ndarray\n        Indices of the training samples used in the final fold.\n    val_indices : np.ndarray\n        Indices of the validation samples used in the final fold.\n    training_length : int\n        Number of epochs completed during the final fold training.\n    num_folds: int\n        number of k-folds\n\n    Notes\n    -----\n    - The function uses 3-fold cross-validation to evaluate generalization.\n    - Preprocessing parameters (alpha, beta, gamma) are randomized for each fold.\n    - This implementation supports only one prediction channel and assumes\n      symmetric treatment of integrals and positivity constraints.\n    - `N_event_pred` and `neutrino_pdfs` are currently not returned meaningfully.\n    \"\"\"\n    (\n        chi_squares,\n        k_fold_losses,\n    ) = (\n        [],\n        [],\n    )\n    x_vals = torch.tensor(x_vals, dtype=torch.float32).view(-1, 1)\n\n    x_int = torch.tensor(x_int, dtype=torch.float32).view(-1, 1)\n\n    indices = np.arange(pred[0].shape[0])\n    k = num_folds\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n\n    folds = []\n    for train_index, test_index in kf.split(indices):\n        folds.append((train_index, test_index))\n\n    for j in range(k):\n        train_indices = folds[j][0]\n        val_size = max(1, int(0.1 * len(train_indices)))\n        val_indices = np.random.choice(train_indices, size=val_size, replace=False)\n        k_fold_indices = folds[j][1]\n\n        alpha, beta, gamma = (\n            np.random.rand() * range_alpha,\n            np.random.rand() * range_beta,\n            np.random.rand() * range_gamma,\n        )\n\n        model = PreprocessedMLP(\n            alpha,\n            beta,\n            gamma,\n            activation_function,\n            hidden_layers,\n            num_input_layers,\n            num_output_layers,\n            preproc,\n        )\n\n        criterion = CustomLoss(\n            extended_loss,\n            num_output_layers,\n        )\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\n        losses = []\n\n        model.train()\n        best_loss = 1e13  # initial loss\n        counter = 0\n        training_length = 0\n\n        while counter &lt; patience:\n            if max_epochs &lt; training_length:\n                break\n\n            training_length += 1\n\n            optimizer.zero_grad()\n            y_pred = model(x_alphas)\n\n            y_preds = torch.matmul(fk_tables, y_pred[:, 0]) * binwidths.flatten()\n            y_preds = y_preds.squeeze()\n            y_int_mu = model(x_int)\n            y_int_mub = y_int_mu\n\n            y_train = y_preds[train_indices]\n            pred_train = pred[0][train_indices]\n            cov_matrix_train = cov_matrix[train_indices][:, train_indices]\n\n            loss = criterion(\n                y_train,\n                pred_train,\n                cov_matrix_train,\n                y_int_mu,\n                y_int_mub,\n                y_pred,\n                x_int,\n                lag_mult_pos,\n                lag_mult_int,\n            )\n            loss.backward()\n\n            y_val = y_preds[val_indices]\n            pred_val = pred[0][val_indices]\n            cov_matrix_val = cov_matrix[val_indices][:, val_indices]\n\n            loss_val = criterion(\n                y_val,\n                pred_val,\n                cov_matrix_val,\n                y_int_mu,\n                y_int_mub,\n                y_pred,\n                x_int,\n                lag_mult_pos,\n                lag_mult_int,\n            )\n\n            if training_length % 500 == 0:\n                chi_squares.append(loss.detach().numpy())\n                print(loss.detach().numpy())\n\n            losses.append(loss.detach().numpy())\n            optimizer.step()\n\n            if loss_val &lt; best_loss:\n                best_loss = loss_val\n                counter = 0\n            else:\n                counter += 1\n\n        y_k_fold = y_preds[k_fold_indices]\n        pred_k_fold = pred[0][k_fold_indices]\n        cov_matrix_k_fold = cov_matrix[k_fold_indices][:, k_fold_indices]\n\n        loss_k_fold = criterion(\n            y_k_fold,\n            pred_k_fold,\n            cov_matrix_k_fold,\n            y_int_mu,\n            y_int_mub,\n            y_pred,\n            x_int,\n            lag_mult_pos,\n            lag_mult_int,\n        )\n\n        k_fold_loss = loss_k_fold.item()\n        k_fold_losses.append(k_fold_loss)\n        print(\"k_fold_losses\")\n        print(k_fold_losses)\n\n    return np.mean(k_fold_losses)\n</code></pre>"},{"location":"api/#hyperopt_nu_nubpy","title":"<code>hyperopt_nu_nub.py</code>","text":""},{"location":"api/#NN_fit.hyperopt_nu_nub.perform_fit","title":"<code>perform_fit(pred, range_alpha, range_beta, range_gamma, lr, wd, patience, x_alphas, fk_tables_mu, fk_tables_mub, binwidths_mu, binwidths_mub, cov_matrix, extended_loss, activation_function, num_input_layers, num_output_layers, hidden_layers, x_vals, preproc, max_epochs, lag_mult_pos, lag_mult_int, x_int, num_folds)</code>","text":"<p>Performs k-fold cross-validation training and evaluation of a neural network for muon neutrino flux prediction using Bayesian-inspired randomized hyperparameters.</p> <p>This function trains a <code>PreprocessedMLP</code> model to predict neutrino and antineutrino event distributions, optimizing a custom loss function that incorporates physical constraints and covariance information. It performs training with early stopping based on validation loss, and evaluates generalization via a held-out fold.</p>"},{"location":"api/#NN_fit.hyperopt_nu_nub.perform_fit--parameters","title":"Parameters:","text":"<p>pred : List[np.ndarray]     List containing ground truth predicted neutrino events for training and evaluation. range_alpha : float     Upper bound for random initialization of the alpha hyperparameter. range_beta : float     Upper bound for random initialization of the beta hyperparameter. range_gamma : float     Upper bound for random initialization of the gamma hyperparameter. lr : float     Learning rate for the optimizer. wd : float     Weight decay (L2 regularization) for the optimizer. patience : int     Number of epochs to wait without validation improvement before early stopping. x_alphas : torch.Tensor     Input tensor used for prediction by the model. fk_tables_mu : torch.Tensor     Forward-folding kernel table for muon neutrinos. fk_tables_mub : torch.Tensor     Forward-folding kernel table for anti-muon neutrinos. binwidths_mu : torch.Tensor     Bin widths used for muon neutrino predictions. binwidths_mub : torch.Tensor     Bin widths used for anti-muon neutrino predictions. cov_matrix : np.ndarray     Covariance matrix for uncertainty propagation in loss computation. extended_loss : bool     Whether to include extended regularization terms in the custom loss. activation_function : str     Activation function to use in the model (e.g., \"relu\", \"tanh\"). num_input_layers : int     Number of input features/layers for the model. num_output_layers : int     Number of outputs (e.g., neutrino types). hidden_layers : List[int]     Sizes of hidden layers in the MLP. x_vals : np.ndarray     Input data values used for training and evaluation. preproc : str     Type of input preprocessing to apply (e.g., \"standard\", \"log\"). max_epochs : int     Maximum number of training epochs per fold. lag_mult_pos : float     Lagrange multiplier weight for positivity constraint in the loss. lag_mult_int : float     Lagrange multiplier weight for integral constraint in the loss. x_int : np.ndarray     Points at which the integrals for regularization are evaluated. num_folds: int     Number of k-folds</p>"},{"location":"api/#NN_fit.hyperopt_nu_nub.perform_fit--returns","title":"Returns:","text":"<p>Tuple[     List[float],           # chi_squares over training     List[np.ndarray],      # Predicted neutrino event counts     List[np.ndarray],      # Predicted neutrino PDFs     PreprocessedMLP,       # Trained model instance     List[float],           # Chi-square values for post-fit analysis     np.ndarray,            # Training indices from final fold     np.ndarray,            # Validation indices from final fold     int                    # Total number of training iterations in last fold ]</p> Source code in <code>NN_fit/hyperopt_nu_nub.py</code> <pre><code>def perform_fit(\n    pred: List[np.ndarray],\n    range_alpha: float,\n    range_beta: float,\n    range_gamma: float,\n    lr: float,\n    wd: float,\n    patience: int,\n    x_alphas: torch.Tensor,\n    fk_tables_mu: torch.Tensor,\n    fk_tables_mub: torch.Tensor,\n    binwidths_mu: torch.Tensor,\n    binwidths_mub: torch.Tensor,\n    cov_matrix: np.ndarray,\n    extended_loss: bool,\n    activation_function: str,\n    num_input_layers: int,\n    num_output_layers: int,\n    hidden_layers: List[int],\n    x_vals: np.ndarray,\n    preproc: str,\n    max_epochs: int,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n    x_int: np.ndarray,\n    num_folds: int,\n) -&gt; Tuple[\n    List[float],  # chi_squares\n    List[np.ndarray],  # N_event_pred\n    List[np.ndarray],  # neutrino_pdfs\n    PreprocessedMLP,  # model (last accepted fit)\n    List[float],  # chi_square_for_postfit\n    np.ndarray,  # train_indices\n    np.ndarray,  # val_indices\n    int,  # training_length\n]:\n    \"\"\"\n    Performs k-fold cross-validation training and evaluation of a neural network\n    for muon neutrino flux prediction using Bayesian-inspired randomized hyperparameters.\n\n    This function trains a `PreprocessedMLP` model to predict neutrino and antineutrino\n    event distributions, optimizing a custom loss function that incorporates physical\n    constraints and covariance information. It performs training with early stopping\n    based on validation loss, and evaluates generalization via a held-out fold.\n\n    Parameters:\n    ----------\n    pred : List[np.ndarray]\n        List containing ground truth predicted neutrino events for training and evaluation.\n    range_alpha : float\n        Upper bound for random initialization of the alpha hyperparameter.\n    range_beta : float\n        Upper bound for random initialization of the beta hyperparameter.\n    range_gamma : float\n        Upper bound for random initialization of the gamma hyperparameter.\n    lr : float\n        Learning rate for the optimizer.\n    wd : float\n        Weight decay (L2 regularization) for the optimizer.\n    patience : int\n        Number of epochs to wait without validation improvement before early stopping.\n    x_alphas : torch.Tensor\n        Input tensor used for prediction by the model.\n    fk_tables_mu : torch.Tensor\n        Forward-folding kernel table for muon neutrinos.\n    fk_tables_mub : torch.Tensor\n        Forward-folding kernel table for anti-muon neutrinos.\n    binwidths_mu : torch.Tensor\n        Bin widths used for muon neutrino predictions.\n    binwidths_mub : torch.Tensor\n        Bin widths used for anti-muon neutrino predictions.\n    cov_matrix : np.ndarray\n        Covariance matrix for uncertainty propagation in loss computation.\n    extended_loss : bool\n        Whether to include extended regularization terms in the custom loss.\n    activation_function : str\n        Activation function to use in the model (e.g., \"relu\", \"tanh\").\n    num_input_layers : int\n        Number of input features/layers for the model.\n    num_output_layers : int\n        Number of outputs (e.g., neutrino types).\n    hidden_layers : List[int]\n        Sizes of hidden layers in the MLP.\n    x_vals : np.ndarray\n        Input data values used for training and evaluation.\n    preproc : str\n        Type of input preprocessing to apply (e.g., \"standard\", \"log\").\n    max_epochs : int\n        Maximum number of training epochs per fold.\n    lag_mult_pos : float\n        Lagrange multiplier weight for positivity constraint in the loss.\n    lag_mult_int : float\n        Lagrange multiplier weight for integral constraint in the loss.\n    x_int : np.ndarray\n        Points at which the integrals for regularization are evaluated.\n    num_folds: int\n        Number of k-folds\n\n    Returns:\n    -------\n    Tuple[\n        List[float],           # chi_squares over training\n        List[np.ndarray],      # Predicted neutrino event counts\n        List[np.ndarray],      # Predicted neutrino PDFs\n        PreprocessedMLP,       # Trained model instance\n        List[float],           # Chi-square values for post-fit analysis\n        np.ndarray,            # Training indices from final fold\n        np.ndarray,            # Validation indices from final fold\n        int                    # Total number of training iterations in last fold\n    ]\n    \"\"\"\n    (\n        chi_squares,\n        k_fold_losses,\n    ) = (\n        [],\n        [],\n    )\n    x_vals = torch.tensor(x_vals, dtype=torch.float32).view(-1, 1)\n    x_int = torch.tensor(x_int, dtype=torch.float32).view(-1, 1)\n\n    indices = np.arange(pred[0].shape[0])\n    k = num_folds\n    kf = KFold(n_splits=k, shuffle=True, random_state=42)\n\n    folds = []\n    for train_index, test_index in kf.split(indices):\n        folds.append((train_index, test_index))\n\n    for j in range(k):\n        train_indices = folds[j][0]\n        val_size = max(1, int(0.1 * len(train_indices)))\n        val_indices = np.random.choice(train_indices, size=val_size, replace=False)\n        k_fold_indices = folds[j][1]\n\n        alpha, beta, gamma = (\n            np.random.rand() * range_alpha,\n            np.random.rand() * range_beta,\n            np.random.rand() * range_gamma,\n        )\n\n        model = PreprocessedMLP(\n            alpha,\n            beta,\n            gamma,\n            activation_function,\n            hidden_layers,\n            num_input_layers,\n            num_output_layers,\n            preproc,\n        )\n\n        criterion = CustomLoss(\n            extended_loss,\n            num_output_layers,\n        )\n\n        optimizer = torch.optim.Adam(model.parameters(), lr=lr, weight_decay=wd)\n\n        losses = []\n\n        model.train()\n        best_loss = 1e13  # initial loss\n        counter = 0\n        training_length = 0\n\n        while counter &lt; patience:\n            if max_epochs &lt; training_length:\n                break\n\n            training_length += 1\n\n            optimizer.zero_grad()\n            y_pred = model(x_alphas)\n\n            y_pred_mu = (\n                torch.matmul(fk_tables_mu, y_pred[:, 0]) * binwidths_mu.flatten()\n            )\n            y_pred_mub = (\n                torch.matmul(fk_tables_mub, y_pred[:, 1]) * binwidths_mub.flatten()\n            )\n\n            y_pred_mu = y_pred_mu.squeeze()\n            y_pred_mub = y_pred_mub.squeeze()\n\n            y_preds = torch.hstack((y_pred_mu, y_pred_mub))\n\n            y_train = y_preds[train_indices]\n            pred_train = pred[0][train_indices]\n            cov_matrix_train = cov_matrix[train_indices][:, train_indices]\n            y_int_mu = model(x_int)[:, 0]\n            y_int_mub = model(x_int)[:, 1]\n\n            loss = criterion(\n                y_train,\n                pred_train,\n                cov_matrix_train,\n                y_int_mu,\n                y_int_mub,\n                y_pred,\n                x_int,\n                lag_mult_pos,\n                lag_mult_int,\n            )\n            loss.backward()\n            # print(loss)\n\n            y_val = y_preds[val_indices]\n            pred_val = pred[0][val_indices]\n            cov_matrix_val = cov_matrix[val_indices][:, val_indices]\n\n            loss_val = criterion(\n                y_val,\n                pred_val,\n                cov_matrix_val,\n                y_int_mu,\n                y_int_mub,\n                y_pred,\n                x_int,\n                lag_mult_pos,\n                lag_mult_int,\n            )\n\n            if training_length % 500 == 0:\n                chi_squares.append(loss.detach().numpy())\n                print(loss.detach().numpy())\n\n            losses.append(loss.detach().numpy())\n            optimizer.step()\n\n            if loss_val &lt; best_loss:\n                best_loss = loss_val\n                counter = 0\n            else:\n                counter += 1\n\n        y_k_fold = y_preds[k_fold_indices]\n        pred_k_fold = pred[0][k_fold_indices]\n        cov_matrix_k_fold = cov_matrix[k_fold_indices][:, k_fold_indices]\n\n        loss_k_fold = criterion(\n            y_k_fold,\n            pred_k_fold,\n            cov_matrix_k_fold,\n            y_int_mu,\n            y_int_mub,\n            y_pred,\n            x_int,\n            lag_mult_pos,\n            lag_mult_int,\n        )\n\n        k_fold_loss = loss_k_fold.item()\n        k_fold_losses.append(k_fold_loss)\n        print(\"k_fold_losses\")\n        print(k_fold_losses)\n\n    return np.mean(k_fold_losses)\n</code></pre>"},{"location":"api/#model-architecture","title":"\ud83e\udde0 Model Architecture","text":""},{"location":"api/#structure_nnpy","title":"<code>structure_NN.py</code>","text":""},{"location":"api/#NN_fit.structure_NN.CustomLoss","title":"<code>CustomLoss</code>","text":"<p>               Bases: <code>Module</code></p> <p>Custom loss function wrapper supporting multiple modes: raw chi-squared, or extended with constraints.</p>"},{"location":"api/#NN_fit.structure_NN.CustomLoss--parameters","title":"Parameters","text":"<p>extended_loss : bool     If True, uses extended loss with positivity and normalization constraints. num_output_layers : int     Determines loss mode: 1 for combined \u03bd+\u03bd\u0304, 2 for separate \u03bd and \u03bd\u0304 losses.</p>"},{"location":"api/#NN_fit.structure_NN.CustomLoss--methods","title":"Methods","text":"<p>forward(pred, data, cov_matrix, small_x_point1, small_x_point2, model, x_int, lag_mult_pos, lag_mult_int)     Computes the loss.</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>class CustomLoss(nn.Module):\n    \"\"\"\n    Custom loss function wrapper supporting multiple modes: raw chi-squared, or extended with constraints.\n\n    Parameters\n    ----------\n    extended_loss : bool\n        If True, uses extended loss with positivity and normalization constraints.\n    num_output_layers : int\n        Determines loss mode: 1 for combined \u03bd+\u03bd\u0304, 2 for separate \u03bd and \u03bd\u0304 losses.\n\n    Methods\n    -------\n    forward(pred, data, cov_matrix, small_x_point1, small_x_point2, model, x_int, lag_mult_pos, lag_mult_int)\n        Computes the loss.\n    \"\"\"\n\n    def __init__(self, extended_loss: bool, num_output_layers: int):\n        super(CustomLoss, self).__init__()\n        self.extended_loss = extended_loss\n        self.num_output_layers = num_output_layers\n\n    def forward(\n        self,\n        pred: torch.Tensor,\n        data: torch.Tensor,\n        cov_matrix: torch.Tensor,\n        small_x_point1: torch.Tensor,\n        small_x_point2: torch.Tensor,\n        model: torch.Tensor,\n        x_int: torch.Tensor,\n        lag_mult_pos: float,\n        lag_mult_int: float,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Computes the loss function.\n\n        Parameters\n        ----------\n        pred : torch.Tensor\n            Model prediction.\n        data : torch.Tensor\n            Target data (pseudo-data).\n        cov_matrix : np.ndarray or torch.Tensor\n            Covariance matrix for data.\n        small_x_point1 : torch.Tensor\n            Neural prediction before preprocessing (\u03bd or combined).\n        small_x_point2 : torch.Tensor\n            Neural prediction before preprocessing (\u03bd\u0304, if using two outputs).\n        model : torch.nn.Module\n            Reference to the model (used for constraints).\n        x_int : torch.Tensor\n            x-points for integral constraint.\n        lag_mult_pos : float\n            Lagrange multiplier for positivity.\n        lag_mult_int : float\n            Lagrange multiplier for integral constraint.\n\n        Returns\n        -------\n        torch.Tensor\n            Final loss scalar.\n        \"\"\"\n        if self.extended_loss:\n            if self.num_output_layers == 1:\n                loss = complete_loss_fct_comb(\n                    pred,\n                    data,\n                    cov_matrix,\n                    small_x_point1,\n                    model,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n            if self.num_output_layers == 2:\n                loss = complete_loss_fct_nu_nub(\n                    pred,\n                    data,\n                    cov_matrix,\n                    small_x_point1,\n                    small_x_point2,\n                    model,\n                    x_int,\n                    lag_mult_pos,\n                    lag_mult_int,\n                )\n\n        else:\n            loss = raw_loss_fct(pred, data, cov_matrix)\n        return loss\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.CustomLoss.forward","title":"<code>forward(pred, data, cov_matrix, small_x_point1, small_x_point2, model, x_int, lag_mult_pos, lag_mult_int)</code>","text":"<p>Computes the loss function.</p>"},{"location":"api/#NN_fit.structure_NN.CustomLoss.forward--parameters","title":"Parameters","text":"<p>pred : torch.Tensor     Model prediction. data : torch.Tensor     Target data (pseudo-data). cov_matrix : np.ndarray or torch.Tensor     Covariance matrix for data. small_x_point1 : torch.Tensor     Neural prediction before preprocessing (\u03bd or combined). small_x_point2 : torch.Tensor     Neural prediction before preprocessing (\u03bd\u0304, if using two outputs). model : torch.nn.Module     Reference to the model (used for constraints). x_int : torch.Tensor     x-points for integral constraint. lag_mult_pos : float     Lagrange multiplier for positivity. lag_mult_int : float     Lagrange multiplier for integral constraint.</p>"},{"location":"api/#NN_fit.structure_NN.CustomLoss.forward--returns","title":"Returns","text":"<p>torch.Tensor     Final loss scalar.</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>def forward(\n    self,\n    pred: torch.Tensor,\n    data: torch.Tensor,\n    cov_matrix: torch.Tensor,\n    small_x_point1: torch.Tensor,\n    small_x_point2: torch.Tensor,\n    model: torch.Tensor,\n    x_int: torch.Tensor,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Computes the loss function.\n\n    Parameters\n    ----------\n    pred : torch.Tensor\n        Model prediction.\n    data : torch.Tensor\n        Target data (pseudo-data).\n    cov_matrix : np.ndarray or torch.Tensor\n        Covariance matrix for data.\n    small_x_point1 : torch.Tensor\n        Neural prediction before preprocessing (\u03bd or combined).\n    small_x_point2 : torch.Tensor\n        Neural prediction before preprocessing (\u03bd\u0304, if using two outputs).\n    model : torch.nn.Module\n        Reference to the model (used for constraints).\n    x_int : torch.Tensor\n        x-points for integral constraint.\n    lag_mult_pos : float\n        Lagrange multiplier for positivity.\n    lag_mult_int : float\n        Lagrange multiplier for integral constraint.\n\n    Returns\n    -------\n    torch.Tensor\n        Final loss scalar.\n    \"\"\"\n    if self.extended_loss:\n        if self.num_output_layers == 1:\n            loss = complete_loss_fct_comb(\n                pred,\n                data,\n                cov_matrix,\n                small_x_point1,\n                model,\n                x_int,\n                lag_mult_pos,\n                lag_mult_int,\n            )\n        if self.num_output_layers == 2:\n            loss = complete_loss_fct_nu_nub(\n                pred,\n                data,\n                cov_matrix,\n                small_x_point1,\n                small_x_point2,\n                model,\n                x_int,\n                lag_mult_pos,\n                lag_mult_int,\n            )\n\n    else:\n        loss = raw_loss_fct(pred, data, cov_matrix)\n    return loss\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.CustomPreprocessing","title":"<code>CustomPreprocessing</code>","text":"<p>               Bases: <code>Module</code></p> <p>Applies a parameterized functional preprocessing to the input based on power-law forms.</p> The form is <p>f(x) = \u03b3 * (1 - x)^\u03b2 * x^(1 - \u03b1)</p>"},{"location":"api/#NN_fit.structure_NN.CustomPreprocessing--parameters","title":"Parameters","text":"<p>alpha : float     Initial value for \u03b1 parameter. beta : float     Initial value for \u03b2 parameter. gamma : float     Initial value for \u03b3 parameter. preproc : bool     If True, alpha, beta, and gamma are learnable parameters. Otherwise, they are fixed.</p>"},{"location":"api/#NN_fit.structure_NN.CustomPreprocessing--notes","title":"Notes","text":"<ul> <li>Input values are clamped between [1e-6, 1 - 1e-6] for numerical stability.</li> </ul> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>class CustomPreprocessing(nn.Module):\n    \"\"\"\n    Applies a parameterized functional preprocessing to the input based on power-law forms.\n\n    The form is:\n        f(x) = \u03b3 * (1 - x)^\u03b2 * x^(1 - \u03b1)\n\n    Parameters\n    ----------\n    alpha : float\n        Initial value for \u03b1 parameter.\n    beta : float\n        Initial value for \u03b2 parameter.\n    gamma : float\n        Initial value for \u03b3 parameter.\n    preproc : bool\n        If True, alpha, beta, and gamma are learnable parameters. Otherwise, they are fixed.\n\n    Notes\n    -----\n    - Input values are clamped between [1e-6, 1 - 1e-6] for numerical stability.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float,\n        beta: float,\n        gamma: float,\n        preproc: bool,\n    ):\n        super(CustomPreprocessing, self).__init__()\n\n        if preproc:\n            self.alpha = nn.Parameter(\n                torch.tensor(alpha, dtype=torch.float32, requires_grad=True)\n            )\n            self.beta = nn.Parameter(\n                torch.tensor(beta, dtype=torch.float32, requires_grad=True)\n            )\n            self.gamma = nn.Parameter(\n                torch.tensor(gamma, dtype=torch.float32, requires_grad=True)\n            )\n        else:\n            self.register_buffer(\"alpha\", torch.tensor(alpha, dtype=torch.float32))\n            self.register_buffer(\"beta\", torch.tensor(beta, dtype=torch.float32))\n            self.register_buffer(\"gamma\", torch.tensor(gamma, dtype=torch.float32))\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Applies the preprocessing function.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (N, 1).\n\n        Returns\n        -------\n        torch.Tensor\n            Preprocessed output of same shape.\n        \"\"\"\n        x = torch.clamp(x, 1e-6, 1 - 1e-6)\n        beta = torch.nn.functional.relu(self.beta)\n\n        return self.gamma * (1 - x) ** beta * x ** (1 - self.alpha)\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.CustomPreprocessing.forward","title":"<code>forward(x)</code>","text":"<p>Applies the preprocessing function.</p>"},{"location":"api/#NN_fit.structure_NN.CustomPreprocessing.forward--parameters","title":"Parameters","text":"<p>x : torch.Tensor     Input tensor of shape (N, 1).</p>"},{"location":"api/#NN_fit.structure_NN.CustomPreprocessing.forward--returns","title":"Returns","text":"<p>torch.Tensor     Preprocessed output of same shape.</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Applies the preprocessing function.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (N, 1).\n\n    Returns\n    -------\n    torch.Tensor\n        Preprocessed output of same shape.\n    \"\"\"\n    x = torch.clamp(x, 1e-6, 1 - 1e-6)\n    beta = torch.nn.functional.relu(self.beta)\n\n    return self.gamma * (1 - x) ** beta * x ** (1 - self.alpha)\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP","title":"<code>PreprocessedMLP</code>","text":"<p>               Bases: <code>Module</code></p> <p>A neural network combining a preprocessing layer with an MLP.</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP--parameters","title":"Parameters","text":"<p>alpha, beta, gamma : float     Parameters for the preprocessing function. activation_function : list of str     Activation functions for the MLP. hidden_layers : list of int     Sizes of hidden layers. num_input_layers : int     Number of input features. num_output_layers : int     Number of output features. preproc : bool     Whether to use preprocessing.</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP--attributes","title":"Attributes","text":"<p>preprocessing : CustomPreprocessing     The preprocessing module applied before the MLP. mlp : SimplePerceptron     The neural network model applied to the preprocessed inputs.</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP--methods","title":"Methods","text":"<p>forward(x)     Applies preprocessing (if enabled) followed by the MLP. neuralnet(x)     Returns raw MLP output (no preprocessing). preproces(x)     Returns the preprocessing factor only.</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>class PreprocessedMLP(nn.Module):\n    \"\"\"\n    A neural network combining a preprocessing layer with an MLP.\n\n    Parameters\n    ----------\n    alpha, beta, gamma : float\n        Parameters for the preprocessing function.\n    activation_function : list of str\n        Activation functions for the MLP.\n    hidden_layers : list of int\n        Sizes of hidden layers.\n    num_input_layers : int\n        Number of input features.\n    num_output_layers : int\n        Number of output features.\n    preproc : bool\n        Whether to use preprocessing.\n\n    Attributes\n    ----------\n    preprocessing : CustomPreprocessing\n        The preprocessing module applied before the MLP.\n    mlp : SimplePerceptron\n        The neural network model applied to the preprocessed inputs.\n\n    Methods\n    -------\n    forward(x)\n        Applies preprocessing (if enabled) followed by the MLP.\n    neuralnet(x)\n        Returns raw MLP output (no preprocessing).\n    preproces(x)\n        Returns the preprocessing factor only.\n    \"\"\"\n\n    def __init__(\n        self,\n        alpha: float,\n        beta: float,\n        gamma: float,\n        activation_function: list[str],\n        hidden_layers: list[int],\n        num_input_layers: int,\n        num_output_layers: int,\n        preproc: bool,\n    ):\n        super(PreprocessedMLP, self).__init__()\n\n        self.preprocessing = CustomPreprocessing(alpha, beta, gamma, preproc)\n        self.mlp = SimplePerceptron(\n            activation_function, num_input_layers, hidden_layers, num_output_layers\n        )\n        self.preproc = preproc\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Full forward pass through preprocessing and MLP.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor.\n\n        Returns\n        -------\n        torch.Tensor\n            Output of the combined preprocessing and MLP.\n        \"\"\"\n        if self.preproc:\n            f_preproc = self.preprocessing(x)\n            f_NN = self.mlp(x)\n            f_nu = f_preproc * f_NN\n            return f_nu\n        else:\n            f_NN = self.mlp(x)\n            return f_NN\n\n    def neuralnet(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through only the MLP (no preprocessing).\n\n        Parameters\n        ----------\n        x : torch.Tensor\n\n        Returns\n        -------\n        torch.Tensor\n        \"\"\"\n        f_NN = self.mlp(x)\n        return f_NN\n\n    def preproces(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Returns the preprocessing term \u03b3 * (1 - x)^\u03b2 * x^(1 - \u03b1)\n\n        Parameters\n        ----------\n        x : torch.Tensor\n\n        Returns\n        -------\n        torch.Tensor\n        \"\"\"\n        f_preproc = self.preprocessing(x)\n        return f_preproc\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.forward","title":"<code>forward(x)</code>","text":"<p>Full forward pass through preprocessing and MLP.</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.forward--parameters","title":"Parameters","text":"<p>x : torch.Tensor     Input tensor.</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output of the combined preprocessing and MLP.</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Full forward pass through preprocessing and MLP.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor.\n\n    Returns\n    -------\n    torch.Tensor\n        Output of the combined preprocessing and MLP.\n    \"\"\"\n    if self.preproc:\n        f_preproc = self.preprocessing(x)\n        f_NN = self.mlp(x)\n        f_nu = f_preproc * f_NN\n        return f_nu\n    else:\n        f_NN = self.mlp(x)\n        return f_NN\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.neuralnet","title":"<code>neuralnet(x)</code>","text":"<p>Forward pass through only the MLP (no preprocessing).</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.neuralnet--parameters","title":"Parameters","text":"<p>x : torch.Tensor</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.neuralnet--returns","title":"Returns","text":"<p>torch.Tensor</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>def neuralnet(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through only the MLP (no preprocessing).\n\n    Parameters\n    ----------\n    x : torch.Tensor\n\n    Returns\n    -------\n    torch.Tensor\n    \"\"\"\n    f_NN = self.mlp(x)\n    return f_NN\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.preproces","title":"<code>preproces(x)</code>","text":"<p>Returns the preprocessing term \u03b3 * (1 - x)^\u03b2 * x^(1 - \u03b1)</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.preproces--parameters","title":"Parameters","text":"<p>x : torch.Tensor</p>"},{"location":"api/#NN_fit.structure_NN.PreprocessedMLP.preproces--returns","title":"Returns","text":"<p>torch.Tensor</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>def preproces(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Returns the preprocessing term \u03b3 * (1 - x)^\u03b2 * x^(1 - \u03b1)\n\n    Parameters\n    ----------\n    x : torch.Tensor\n\n    Returns\n    -------\n    torch.Tensor\n    \"\"\"\n    f_preproc = self.preprocessing(x)\n    return f_preproc\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron","title":"<code>SimplePerceptron</code>","text":"<p>               Bases: <code>Module</code></p> <p>A feedforward multilayer perceptron (MLP) with configurable activation functions and layer sizes.</p>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron--parameters","title":"Parameters","text":"<p>act_functions : list of str     List of activation function names (e.g., ['relu', 'relu', 'softplus']) for each layer. num_input_layers : int     Number of input features. hidden_layers : list of int     List of integers specifying the number of units in each hidden layer. num_output_layers : int     Number of output features.</p>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron--attributes","title":"Attributes","text":"<p>layers : nn.Sequential     Composed list of linear and activation layers forming the MLP.</p>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron--notes","title":"Notes","text":"<ul> <li>Supported activation functions: 'relu', 'softplus'.</li> <li>The last activation is applied after the final output layer.</li> </ul> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>class SimplePerceptron(torch.nn.Module):\n    \"\"\"\n    A feedforward multilayer perceptron (MLP) with configurable activation functions and layer sizes.\n\n    Parameters\n    ----------\n    act_functions : list of str\n        List of activation function names (e.g., ['relu', 'relu', 'softplus']) for each layer.\n    num_input_layers : int\n        Number of input features.\n    hidden_layers : list of int\n        List of integers specifying the number of units in each hidden layer.\n    num_output_layers : int\n        Number of output features.\n\n    Attributes\n    ----------\n    layers : nn.Sequential\n        Composed list of linear and activation layers forming the MLP.\n\n    Notes\n    -----\n    - Supported activation functions: 'relu', 'softplus'.\n    - The last activation is applied after the final output layer.\n    \"\"\"\n\n    def __init__(\n        self,\n        act_functions: list[str],\n        num_input_layers: int,\n        hidden_layers: list[int],\n        num_output_layers: int,\n    ):\n        super().__init__()\n\n        activation_map = {\n            \"softplus\": nn.Softplus,\n            \"relu\": nn.ReLU,\n            \"tanh\": nn.Tanh,\n            \"sigmoid\": nn.Sigmoid,\n        }\n\n        activation_names = act_functions\n\n        act_functions = [activation_map[name]() for name in activation_names]\n\n        layers = []\n\n        layers.append(nn.Linear(num_input_layers, hidden_layers[0]))\n        layers.append(act_functions[0])\n\n        for i in range(0, len(hidden_layers) - 1):\n            layers.append(nn.Linear(hidden_layers[i], hidden_layers[i + 1]))\n            layers.append(act_functions[i])\n\n        layers.append(nn.Linear(hidden_layers[-1], num_output_layers))\n        layers.append(act_functions[-1])\n\n        self.layers = nn.Sequential(*layers)\n\n    def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n        \"\"\"\n        Forward pass through the MLP.\n\n        Parameters\n        ----------\n        x : torch.Tensor\n            Input tensor of shape (batch_size, num_input_layers)\n\n        Returns\n        -------\n        torch.Tensor\n            Output tensor of shape (batch_size, num_output_layers)\n        \"\"\"\n        return self.layers(x)\n</code></pre>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron.forward","title":"<code>forward(x)</code>","text":"<p>Forward pass through the MLP.</p>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron.forward--parameters","title":"Parameters","text":"<p>x : torch.Tensor     Input tensor of shape (batch_size, num_input_layers)</p>"},{"location":"api/#NN_fit.structure_NN.SimplePerceptron.forward--returns","title":"Returns","text":"<p>torch.Tensor     Output tensor of shape (batch_size, num_output_layers)</p> Source code in <code>NN_fit/structure_NN.py</code> <pre><code>def forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"\n    Forward pass through the MLP.\n\n    Parameters\n    ----------\n    x : torch.Tensor\n        Input tensor of shape (batch_size, num_input_layers)\n\n    Returns\n    -------\n    torch.Tensor\n        Output tensor of shape (batch_size, num_output_layers)\n    \"\"\"\n    return self.layers(x)\n</code></pre>"},{"location":"api/#plotting-and-visualization","title":"\ud83d\udcc8 Plotting and Visualization","text":""},{"location":"api/#plot_comb_pdf_clpy","title":"<code>plot_comb_pdf_cl.py</code>","text":""},{"location":"api/#plot_diff_level1_combpy","title":"<code>plot_diff_level1_comb.py</code>","text":""},{"location":"api/#plot_for_diff_level_1_shifts_nu_nubpy","title":"<code>plot_for_diff_level_1_shifts_nu_nub.py</code>","text":""},{"location":"api/#plot_nu_nub_clpy","title":"<code>plot_nu_nub_cl.py</code>","text":""},{"location":"api/#pdf-output","title":"\ud83d\udce6 PDF Output","text":""},{"location":"api/#write_all_pdfs_to_lhapdfpy","title":"<code>write_all_pdfs_to_lhapdf.py</code>","text":""},{"location":"api/#NN_fit.write_all_pdfs_to_lhapdf.customize_info_file","title":"<code>customize_info_file(template_path, output_path, set_index, flavor, num_members)</code>","text":"<p>Creates a customized LHAPDF <code>.info</code> file from a template by replacing placeholders.</p>"},{"location":"api/#NN_fit.write_all_pdfs_to_lhapdf.customize_info_file--parameters","title":"Parameters:","text":"<p>template_path : str     Path to the <code>.info</code> template file containing placeholders (e.g., SETINDEX, FLAVOR). output_path : str     Path to the output <code>.info</code> file to be generated. set_index : int     Unique identifier for the PDF set (used to replace \"SETINDEX\" in the template). flavor : str     Flavor content to be listed in the info file (used to replace \"FLAVOR\").     Can be a single PDG ID (e.g., \"12\") or a comma-separated list (e.g., \"14, -14\"). num_members : int     Number of PDF members or replicas (used to replace the \"NumMembers\" field).</p>"},{"location":"api/#NN_fit.write_all_pdfs_to_lhapdf.customize_info_file--notes","title":"Notes:","text":"<ul> <li>The function assumes the template has default \"NumMembers: 1000\" and replaces that value.</li> <li>All replaced content is written to the specified output path.</li> </ul> Source code in <code>NN_fit/write_all_pdfs_to_lhapdf.py</code> <pre><code>def customize_info_file(\n    template_path: str, output_path: str, set_index: int, flavor: str, num_members: int\n) -&gt; None:\n    \"\"\"\n    Creates a customized LHAPDF `.info` file from a template by replacing placeholders.\n\n    Parameters:\n    -----------\n    template_path : str\n        Path to the `.info` template file containing placeholders (e.g., SETINDEX, FLAVOR).\n    output_path : str\n        Path to the output `.info` file to be generated.\n    set_index : int\n        Unique identifier for the PDF set (used to replace \"SETINDEX\" in the template).\n    flavor : str\n        Flavor content to be listed in the info file (used to replace \"FLAVOR\").\n        Can be a single PDG ID (e.g., \"12\") or a comma-separated list (e.g., \"14, -14\").\n    num_members : int\n        Number of PDF members or replicas (used to replace the \"NumMembers\" field).\n\n    Notes:\n    ------\n    - The function assumes the template has default \"NumMembers: 1000\" and replaces that value.\n    - All replaced content is written to the specified output path.\n    \"\"\"\n    with open(template_path, \"r\") as file:\n        content = file.read()\n\n    content = content.replace(\"SETINDEX\", str(set_index))\n    content = content.replace(\"FLAVOR\", str(flavor))\n    content = content.replace(\"NumMembers: 1000\", f\"NumMembers: {str(num_members)}\")\n\n    with open(output_path, \"w\") as file:\n        file.write(content)\n</code></pre>"},{"location":"api/#NN_fit.write_all_pdfs_to_lhapdf.write_lhapdf_grid","title":"<code>write_lhapdf_grid(xgrid, pdf_dict, path)</code>","text":"<p>Writes a set of neutrino PDFs to a file in LHAPDF grid format (lhagrid1).</p> <p>This function formats and saves PDF data into a file readable by LHAPDF tools, using the specified x-grid and dictionary of parton distribution functions.</p>"},{"location":"api/#NN_fit.write_all_pdfs_to_lhapdf.write_lhapdf_grid--parameters","title":"Parameters:","text":"<p>xgrid : array-like     Array of Bjorken-x values at which PDFs are evaluated. pdf_dict : dict     Dictionary mapping particle IDs (PDG codes) to arrays of PDF values.     Each value should be an array of length equal to the length of <code>xgrid</code>. path : str     Path to the output <code>.dat</code> file where the grid will be written.</p>"},{"location":"api/#NN_fit.write_all_pdfs_to_lhapdf.write_lhapdf_grid--notes","title":"Notes:","text":"<ul> <li>The output format follows LHAPDF's <code>lhagrid1</code> specification.</li> <li>Each PDF line is duplicated, as required by the LHAPDF grid format.</li> <li>The order of flavors is sorted by PDG code.</li> </ul> Source code in <code>NN_fit/write_all_pdfs_to_lhapdf.py</code> <pre><code>def write_lhapdf_grid(\n    xgrid: Union[np.ndarray, List[float]],\n    pdf_dict: Dict[int, Union[np.ndarray, List[float]]],\n    path: str,\n) -&gt; None:\n    \"\"\"\n    Writes a set of neutrino PDFs to a file in LHAPDF grid format (lhagrid1).\n\n    This function formats and saves PDF data into a file readable by LHAPDF tools,\n    using the specified x-grid and dictionary of parton distribution functions.\n\n    Parameters:\n    -----------\n    xgrid : array-like\n        Array of Bjorken-x values at which PDFs are evaluated.\n    pdf_dict : dict\n        Dictionary mapping particle IDs (PDG codes) to arrays of PDF values.\n        Each value should be an array of length equal to the length of `xgrid`.\n    path : str\n        Path to the output `.dat` file where the grid will be written.\n\n    Notes:\n    ------\n    - The output format follows LHAPDF's `lhagrid1` specification.\n    - Each PDF line is duplicated, as required by the LHAPDF grid format.\n    - The order of flavors is sorted by PDG code.\n    \"\"\"\n    with open(path, \"w\") as f:\n        f.write(\"PdfType: replica\\n\")\n        f.write(\"Format: lhagrid1\\n\")\n        f.write(\"---\\n\")\n\n        f.write(\"  \" + \"  \".join(f\"{val:.8e}\" for val in xgrid) + \"\\n\")\n\n        f.write(\"0.1E+001 0.1E+007\\n\")\n\n        pids = sorted(pdf_dict.keys())\n        f.write(\" \".join(str(pid) for pid in pids) + \"\\n\")\n\n        num_x = len(xgrid)\n        for i in range(num_x):\n            line = \" \".join(f\"{pdf_dict[pid][i]:.14e}\" for pid in pids)\n            f.write(line + \"\\n\")\n            f.write(line + \"\\n\")\n        f.write(\"---\\n\")\n</code></pre>"},{"location":"api/#data-and-reading","title":"\ud83e\uddea Data and Reading","text":""},{"location":"api/#mc_data_repspy","title":"<code>MC_data_reps.py</code>","text":""},{"location":"api/#NN_fit.MC_data_reps.generate_MC_replicas","title":"<code>generate_MC_replicas(REPLICAS, data, sig_sys, sig_stat, seed)</code>","text":"<p>Generate level 2 data MC replicas for the NN fit by adding a level 1 and then a level 2 gaussian noise to the data</p> <p>Returns:</p> Name Type Description <code>tuple</code> <code>Tuple[List[Tensor], List[Tensor], List[Tensor]]</code> <p>level 0,1 and 2 data</p> Source code in <code>NN_fit/MC_data_reps.py</code> <pre><code>def generate_MC_replicas(\n    REPLICAS: int,\n    data: np.ndarray,\n    sig_sys: np.ndarray,\n    sig_stat: np.ndarray,\n    seed: int,\n) -&gt; Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n    \"\"\"Generate level 2 data MC replicas for the NN fit by adding a level 1 and then a level 2 gaussian noise to the data\n\n    Returns:\n        tuple: level 0,1 and 2 data\n    \"\"\"\n\n    level0, level1, level2 = [], [], []\n    rng_level1 = np.random.default_rng(seed=seed)\n    r_sys_1 = rng_level1.normal(0, 1, len(data)) * sig_sys\n    r_stat_1 = rng_level1.normal(0, 1, len(data)) * sig_stat\n\n    for _ in range(REPLICAS):\n        data_level1 = data + r_sys_1 + r_stat_1\n        r_sys_2 = np.random.normal(0, 1, len(data)) * sig_sys\n        r_stat_2 = np.random.normal(0, 1, len(data)) * sig_stat\n\n        data_level2 = data_level1 + r_sys_2 + r_stat_2\n\n        level0.append(torch.tensor(data, dtype=torch.float32))\n        level1.append(torch.tensor(data_level1, dtype=torch.float32))\n        level2.append(torch.tensor(data_level2, dtype=torch.float32))\n\n    return level0, level1, level2\n</code></pre>"},{"location":"api/#read_faserv_pdfpy","title":"<code>read_faserv_pdf.py</code>","text":""},{"location":"api/#NN_fit.read_faserv_pdf.read_pdf","title":"<code>read_pdf(pdf, x_vals, particle, set)</code>","text":"<p>Reads the parton distribution function (PDF) values for a given particle at specified momentum fractions and energy scale using LHAPDF.</p>"},{"location":"api/#NN_fit.read_faserv_pdf.read_pdf--parameters","title":"Parameters","text":"<p>pdf : str     Name of the PDF set to load. x_vals : np.ndarray     Array of momentum fraction values (x) at which to evaluate the PDF. particle : int     Particle ID (PDG code) for which the PDF is evaluated. set : int     Specific member or set number within the PDF.</p>"},{"location":"api/#NN_fit.read_faserv_pdf.read_pdf--returns","title":"Returns","text":"<p>Tuple[np.ndarray, np.ndarray]     A tuple containing:     - pdf_vals: np.ndarray of PDF values normalized by x_vals.     - x_vals: The input array of momentum fractions.</p> Source code in <code>NN_fit/read_faserv_pdf.py</code> <pre><code>def read_pdf(\n    pdf: str, x_vals: np.ndarray, particle: int, set: int\n) -&gt; Tuple[np.ndarray, np.ndarray]:\n    \"\"\"\n    Reads the parton distribution function (PDF) values for a given particle\n    at specified momentum fractions and energy scale using LHAPDF.\n\n    Parameters\n    ----------\n    pdf : str\n        Name of the PDF set to load.\n    x_vals : np.ndarray\n        Array of momentum fraction values (x) at which to evaluate the PDF.\n    particle : int\n        Particle ID (PDG code) for which the PDF is evaluated.\n    set : int\n        Specific member or set number within the PDF.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray]\n        A tuple containing:\n        - pdf_vals: np.ndarray of PDF values normalized by x_vals.\n        - x_vals: The input array of momentum fractions.\n    \"\"\"\n    pid = particle\n    Q2 = 10\n    pdf = lhapdf.mkPDF(pdf, set)\n    pdf_vals = [pdf.xfxQ2(pid, x, Q2) for x in x_vals]\n    pdf_vals = np.array(pdf_vals)\n    pdf_vals /= x_vals\n    return pdf_vals, x_vals\n</code></pre>"},{"location":"api/#read_fk_tablepy","title":"<code>read_fk_table.py</code>","text":""},{"location":"api/#NN_fit.read_fk_table.get_fk_table","title":"<code>get_fk_table(filename, parent_dir)</code>","text":"<p>Loads a FastKernel (FK) table and corresponding x_alpha nodes from text files, converts them to PyTorch tensors, and reshapes x_alpha to a column vector.</p>"},{"location":"api/#NN_fit.read_fk_table.get_fk_table--parameters","title":"Parameters","text":"<p>filename : str     Name of the FK table file to load (relative to parent_dir). parent_dir : str     Path to the directory containing the FK table file and relative location     of the x_alpha file.</p>"},{"location":"api/#NN_fit.read_fk_table.get_fk_table--returns","title":"Returns","text":"<p>Tuple[torch.Tensor, torch.Tensor]     - x_alpha: Tensor of shape (N, 1) containing x_alpha grid nodes.     - fk_table: Tensor containing the FK table data.</p> Source code in <code>NN_fit/read_fk_table.py</code> <pre><code>def get_fk_table(filename: str, parent_dir: str) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"\n    Loads a FastKernel (FK) table and corresponding x_alpha nodes from text files,\n    converts them to PyTorch tensors, and reshapes x_alpha to a column vector.\n\n    Parameters\n    ----------\n    filename : str\n        Name of the FK table file to load (relative to parent_dir).\n    parent_dir : str\n        Path to the directory containing the FK table file and relative location\n        of the x_alpha file.\n\n    Returns\n    -------\n    Tuple[torch.Tensor, torch.Tensor]\n        - x_alpha: Tensor of shape (N, 1) containing x_alpha grid nodes.\n        - fk_table: Tensor containing the FK table data.\n    \"\"\"\n    file_path = os.path.join(parent_dir, filename)\n    fk_table = np.loadtxt(file_path, delimiter=None)\n\n    file_path = os.path.join(parent_dir, \"../../../Data/gridnodes/x_alpha.dat\")\n    x_alpha = np.loadtxt(file_path, delimiter=None)\n\n    x_alpha = torch.tensor(x_alpha, dtype=torch.float32).view(-1, 1)\n    fk_table = torch.tensor(fk_table, dtype=torch.float32)\n\n    return x_alpha, fk_table\n</code></pre>"},{"location":"api/#help_read_filespy","title":"<code>help_read_files.py</code>","text":""},{"location":"api/#post-fit-analysis","title":"\ud83d\udcca Post-Fit Analysis","text":""},{"location":"api/#postfit_criteriapy","title":"<code>postfit_criteria.py</code>","text":""},{"location":"api/#NN_fit.postfit_criteria.Postfit","title":"<code>Postfit</code>","text":"Source code in <code>NN_fit/postfit_criteria.py</code> <pre><code>class Postfit:\n    def __init__(self) -&gt; None:\n        pass\n\n    # def compute_arc_length(self, model):\n    #     npoints = 199  # 200 intervals\n    #     seg_min = [1e-6, 1e-4, 1e-2]\n    #     seg_max = [1e-4, 1e-2, 1.0]\n    #     res = 0\n    #     for a, b in zip(seg_min, seg_max):\n    #         eps = (b - a) / npoints\n    #         ixgrid = np.linspace(a, b, npoints, endpoint=False)\n    #         ixgrid = torch.tensor(ixgrid, dtype=torch.float32).view(-1, 1)\n\n    #         pdf_vals_grid = model(ixgrid)\n    #         pdf_vals_grid = pdf_vals_grid.detach().numpy().flatten()\n    #         ixgrid = ixgrid.detach().numpy().flatten()\n\n    #         fdiff = np.diff(pdf_vals_grid) / eps\n    #         res += integrate.simpson(np.sqrt(1 + np.square(fdiff)), x=ixgrid[1:])\n    #     return res\n\n    def apply_postfit_criteria(\n        self,\n        chi_squares: List[float],\n        N_event_pred: np.ndarray,\n        neutrino_pdfs: np.ndarray,\n        pred: np.ndarray,\n    ) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n        \"\"\"\n        Applies post-fit criteria to filter out replicas with chi-squared values\n        that deviate significantly from the mean (more than 4 standard deviations).\n\n        Parameters\n        ----------\n        chi_squares : List[float]\n            List of chi-squared values for each replica.\n        N_event_pred : np.ndarray\n            Array of predicted event yields for each replica.\n        neutrino_pdfs : np.ndarray\n            Array of predicted neutrino PDFs for each replica.\n        pred : np.ndarray\n            Array of original pseudo-data predictions for each replica.\n\n        Returns\n        -------\n        Tuple[np.ndarray, np.ndarray, np.ndarray]\n            Filtered arrays of neutrino_pdfs, N_event_pred, and pred with outlier replicas removed.\n\n        Notes\n        -----\n        Replicas whose chi-squared differ from the mean by more than 4 standard deviations\n        are considered outliers and removed.\n        \"\"\"\n        sig_chi_squares = np.std(chi_squares)\n        mean_chi_squares = np.mean(chi_squares)\n\n        indices_to_remove = []\n        for i in range(len(chi_squares)):\n            diff_chi_squares = abs(chi_squares[i] - mean_chi_squares)\n            if diff_chi_squares &gt; 4 * sig_chi_squares:\n                indices_to_remove.append(i)\n\n        if len(indices_to_remove) &gt; 0:\n            indices_to_remove = np.array(indices_to_remove)\n            N_event_pred = np.delete(N_event_pred, indices_to_remove)\n            neutrino_pdfs = np.delete(neutrino_pdfs, indices_to_remove)\n            pred = np.delete(pred, indices_to_remove)\n\n        return neutrino_pdfs, N_event_pred, pred\n</code></pre>"},{"location":"api/#NN_fit.postfit_criteria.Postfit.apply_postfit_criteria","title":"<code>apply_postfit_criteria(chi_squares, N_event_pred, neutrino_pdfs, pred)</code>","text":"<p>Applies post-fit criteria to filter out replicas with chi-squared values that deviate significantly from the mean (more than 4 standard deviations).</p>"},{"location":"api/#NN_fit.postfit_criteria.Postfit.apply_postfit_criteria--parameters","title":"Parameters","text":"<p>chi_squares : List[float]     List of chi-squared values for each replica. N_event_pred : np.ndarray     Array of predicted event yields for each replica. neutrino_pdfs : np.ndarray     Array of predicted neutrino PDFs for each replica. pred : np.ndarray     Array of original pseudo-data predictions for each replica.</p>"},{"location":"api/#NN_fit.postfit_criteria.Postfit.apply_postfit_criteria--returns","title":"Returns","text":"<p>Tuple[np.ndarray, np.ndarray, np.ndarray]     Filtered arrays of neutrino_pdfs, N_event_pred, and pred with outlier replicas removed.</p>"},{"location":"api/#NN_fit.postfit_criteria.Postfit.apply_postfit_criteria--notes","title":"Notes","text":"<p>Replicas whose chi-squared differ from the mean by more than 4 standard deviations are considered outliers and removed.</p> Source code in <code>NN_fit/postfit_criteria.py</code> <pre><code>def apply_postfit_criteria(\n    self,\n    chi_squares: List[float],\n    N_event_pred: np.ndarray,\n    neutrino_pdfs: np.ndarray,\n    pred: np.ndarray,\n) -&gt; Tuple[np.ndarray, np.ndarray, np.ndarray]:\n    \"\"\"\n    Applies post-fit criteria to filter out replicas with chi-squared values\n    that deviate significantly from the mean (more than 4 standard deviations).\n\n    Parameters\n    ----------\n    chi_squares : List[float]\n        List of chi-squared values for each replica.\n    N_event_pred : np.ndarray\n        Array of predicted event yields for each replica.\n    neutrino_pdfs : np.ndarray\n        Array of predicted neutrino PDFs for each replica.\n    pred : np.ndarray\n        Array of original pseudo-data predictions for each replica.\n\n    Returns\n    -------\n    Tuple[np.ndarray, np.ndarray, np.ndarray]\n        Filtered arrays of neutrino_pdfs, N_event_pred, and pred with outlier replicas removed.\n\n    Notes\n    -----\n    Replicas whose chi-squared differ from the mean by more than 4 standard deviations\n    are considered outliers and removed.\n    \"\"\"\n    sig_chi_squares = np.std(chi_squares)\n    mean_chi_squares = np.mean(chi_squares)\n\n    indices_to_remove = []\n    for i in range(len(chi_squares)):\n        diff_chi_squares = abs(chi_squares[i] - mean_chi_squares)\n        if diff_chi_squares &gt; 4 * sig_chi_squares:\n            indices_to_remove.append(i)\n\n    if len(indices_to_remove) &gt; 0:\n        indices_to_remove = np.array(indices_to_remove)\n        N_event_pred = np.delete(N_event_pred, indices_to_remove)\n        neutrino_pdfs = np.delete(neutrino_pdfs, indices_to_remove)\n        pred = np.delete(pred, indices_to_remove)\n\n    return neutrino_pdfs, N_event_pred, pred\n</code></pre>"},{"location":"api/#postfit_measurespy","title":"<code>postfit_measures.py</code>","text":""},{"location":"api/#NN_fit.postfit_measures.Measures","title":"<code>Measures</code>","text":"Source code in <code>NN_fit/postfit_measures.py</code> <pre><code>class Measures:\n    def __init__(\n        self,\n        cov_matrix: torch.Tensor,\n        pdf: np.ndarray,\n        N_event_pred: np.ndarray,\n    ) -&gt; None:\n        self.cov_matrix = cov_matrix\n        self.pdf = pdf\n        self.N_event_pred = N_event_pred\n        \"\"\"\n        Initialize Measures class with covariance matrix, PDF, and predicted events.\n\n        Parameters\n        ----------\n        cov_matrix : torch.Tensor\n            Covariance matrix used in chi-squared calculations.\n        pdf : np.ndarray\n            Reference PDF array.\n        N_event_pred : np.ndarray\n            Predicted event yields array, shape (num_replicas, num_bins).\n        \"\"\"\n\n    def compute_delta_chi(\n        self,\n        level0: Union[np.ndarray, torch.Tensor],\n        N_event_pred: np.ndarray,\n        data_level1: torch.Tensor,\n        x_vals: Union[np.ndarray, torch.Tensor],\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the relative change in chi-squared between a baseline theory prediction\n        and the fit prediction.\n\n        Parameters\n        ----------\n        level0 : Union[np.ndarray, torch.Tensor]\n            Baseline theory prediction.\n        N_event_pred : np.ndarray\n            Predicted events for all replicas.\n        data_level1 : torch.Tensor\n            Observed data level 1 tensor, shape (num_bins, 1).\n        x_vals : Union[np.ndarray, torch.Tensor]\n            x-values (unused in this function).\n\n        Returns\n        -------\n        torch.Tensor\n            Relative change in chi-squared (delta chi).\n        \"\"\"\n        level0 = torch.tensor(level0, dtype=torch.float32)\n        data_level1 = data_level1.view(-1, 1)\n        diff = level0 - data_level1\n\n        diffcov = torch.matmul(self.cov_matrix, diff)\n        chi_theory = torch.dot(diff.view(-1), diffcov.view(-1))\n\n        mean_N_events = np.mean(N_event_pred, axis=0)\n        mean_N_events = torch.tensor(mean_N_events, dtype=torch.float32)\n        diff = mean_N_events - data_level1[0]\n\n        diffcov = torch.matmul(self.cov_matrix, diff)\n        chi_fit = torch.dot(diff.view(-1), diffcov.view(-1))\n\n        delta_chi = (chi_fit - chi_theory) / chi_theory\n\n        return delta_chi\n\n    def compute_phi(\n        self,\n        data: Union[np.ndarray, torch.Tensor],\n        chi_squares: List[float],\n    ) -&gt; float:\n        \"\"\"\n        Compute the phi metric as the difference between average chi-square over replicas\n        and chi-square of the mean prediction.\n\n        Parameters\n        ----------\n        data : Union[np.ndarray, torch.Tensor]\n            Observed data points.\n        chi_squares : List[float]\n            List of chi-square losses per replica.\n\n        Returns\n        -------\n        float\n            The phi metric.\n        \"\"\"\n        num_reps = self.N_event_pred.shape[0]\n        data = torch.tensor(data, dtype=torch.float32)\n        chis = []\n\n        for i in range(num_reps):\n            N_event_rep = torch.tensor(self.N_event_pred[i], dtype=torch.float32)\n\n            diff = N_event_rep - data\n            diffcov = torch.matmul(self.cov_matrix, diff)\n            loss = torch.dot(diff.view(-1), diffcov.view(-1))\n            chis.append(loss)\n        mean_N_event_fits = np.mean(self.N_event_pred, axis=0)\n\n        # data = torch.tensor(data, dtype=torch.float32)\n        mean_N_event_fits = torch.tensor(mean_N_event_fits, dtype=torch.float32)\n\n        diff = mean_N_event_fits - data\n        diffcov = torch.matmul(self.cov_matrix, diff)\n        mean = torch.dot(diff.view(-1), diffcov.view(-1))\n\n        phi_chi = np.mean(chis) - mean\n        return phi_chi\n\n    def compute_accuracy(\n        self,\n        x_alphas: np.ndarray,\n        neutrino_pdf: np.ndarray,\n        pdf: str,\n        n: float,\n        pdf_set: str,\n        pid: int,\n    ) -&gt; float:\n        \"\"\"\n        Compute the accuracy metric as the fraction of bins where the predicted neutrino PDF\n        agrees with the reference PDF within n standard deviations.\n\n        Parameters\n        ----------\n        x_alphas : np.ndarray\n            Input x-values (not used directly in this function).\n        neutrino_pdf : np.ndarray\n            Array of predicted neutrino PDFs (shape: replicas x bins).\n        pdf : str\n            Path or identifier for the reference PDF file.\n        n : float\n            Number of standard deviations for the acceptance criterion.\n        pdf_set : str\n            Identifier of the PDF set.\n        pid : int\n            Particle ID for PDF retrieval.\n\n        Returns\n        -------\n        float\n            Fraction of bins where predicted PDF agrees within n std deviations.\n        \"\"\"\n        mean_neutrino_pdf = np.mean(neutrino_pdf, axis=0)\n        std_pdfs = np.std(neutrino_pdf, axis=0)\n        distances = []\n\n        arr = np.logspace(-5, 0, 1000)\n\n        log_vals = np.logspace(np.log10(0.02), np.log10(0.8), 100)\n        lin_vals = np.linspace(0.1, 0.8, 10)\n        log_indices = np.searchsorted(arr, log_vals)\n        lin_indices = np.searchsorted(arr, lin_vals)\n        indices = np.concatenate((log_indices, lin_indices))\n\n        faser_pdf, x_faser = read_pdf(pdf, arr, pid, pdf_set)\n        faser_pdf = faser_pdf.flatten() * 1.16186e-09\n        faser_pdf = faser_pdf[indices]\n        mean_neutrino_pdf = mean_neutrino_pdf[indices]\n\n        std_pdfs = std_pdfs[indices]\n\n        for i in range(len(indices)):\n            if abs(mean_neutrino_pdf[i] - faser_pdf[i]) &lt; n * std_pdfs[i]:\n                distances.append(1)\n\n        distance = len(distances) / len(indices)\n\n        return distance\n\n    def compute_bias_to_variance(\n        self,\n        level0: Union[np.ndarray, torch.Tensor],\n        level2: np.ndarray,\n        N_event_pred: np.ndarray,\n        REPLICAS: int,\n    ) -&gt; torch.Tensor:\n        \"\"\"\n        Compute the ratio of bias to variance in the PDF fits.\n\n        Parameters\n        ----------\n        level0 : Union[np.ndarray, torch.Tensor]\n            Baseline theory prediction.\n        level2 : np.ndarray\n            Array of predictions at level 2, shape (REPLICAS, num_bins).\n        N_event_pred : np.ndarray\n            Predicted events for all replicas.\n        REPLICAS : int\n            Number of replicas.\n\n        Returns\n        -------\n        torch.Tensor\n            Ratio of bias to variance.\n        \"\"\"\n        mean_N_events = np.mean(N_event_pred, axis=0)\n        mean_N_events = torch.tensor(mean_N_events, dtype=torch.float32).view(-1)\n\n        level0 = torch.tensor(level0, dtype=torch.float32).view(-1)\n\n        diff = mean_N_events - level0\n        diffcov = torch.matmul(self.cov_matrix, diff)\n        bias = torch.dot(diff.view(-1), diffcov.view(-1))\n\n        chi_square_level2 = 0\n\n        for j in range(REPLICAS):\n            diff = mean_N_events - level2[j]\n            diff = diff.float()\n            diffcov = torch.matmul(self.cov_matrix, diff)\n            chi_square = torch.dot(diff.view(-1), diffcov.view(-1))\n\n            chi_square_level2 += chi_square\n\n        variance = chi_square_level2 / REPLICAS\n\n        ratio = bias / variance\n        return ratio\n</code></pre>"},{"location":"api/#NN_fit.postfit_measures.Measures.N_event_pred","title":"<code>N_event_pred = N_event_pred</code>  <code>instance-attribute</code>","text":"<p>Initialize Measures class with covariance matrix, PDF, and predicted events.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.N_event_pred--parameters","title":"Parameters","text":"<p>cov_matrix : torch.Tensor     Covariance matrix used in chi-squared calculations. pdf : np.ndarray     Reference PDF array. N_event_pred : np.ndarray     Predicted event yields array, shape (num_replicas, num_bins).</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_accuracy","title":"<code>compute_accuracy(x_alphas, neutrino_pdf, pdf, n, pdf_set, pid)</code>","text":"<p>Compute the accuracy metric as the fraction of bins where the predicted neutrino PDF agrees with the reference PDF within n standard deviations.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_accuracy--parameters","title":"Parameters","text":"<p>x_alphas : np.ndarray     Input x-values (not used directly in this function). neutrino_pdf : np.ndarray     Array of predicted neutrino PDFs (shape: replicas x bins). pdf : str     Path or identifier for the reference PDF file. n : float     Number of standard deviations for the acceptance criterion. pdf_set : str     Identifier of the PDF set. pid : int     Particle ID for PDF retrieval.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_accuracy--returns","title":"Returns","text":"<p>float     Fraction of bins where predicted PDF agrees within n std deviations.</p> Source code in <code>NN_fit/postfit_measures.py</code> <pre><code>def compute_accuracy(\n    self,\n    x_alphas: np.ndarray,\n    neutrino_pdf: np.ndarray,\n    pdf: str,\n    n: float,\n    pdf_set: str,\n    pid: int,\n) -&gt; float:\n    \"\"\"\n    Compute the accuracy metric as the fraction of bins where the predicted neutrino PDF\n    agrees with the reference PDF within n standard deviations.\n\n    Parameters\n    ----------\n    x_alphas : np.ndarray\n        Input x-values (not used directly in this function).\n    neutrino_pdf : np.ndarray\n        Array of predicted neutrino PDFs (shape: replicas x bins).\n    pdf : str\n        Path or identifier for the reference PDF file.\n    n : float\n        Number of standard deviations for the acceptance criterion.\n    pdf_set : str\n        Identifier of the PDF set.\n    pid : int\n        Particle ID for PDF retrieval.\n\n    Returns\n    -------\n    float\n        Fraction of bins where predicted PDF agrees within n std deviations.\n    \"\"\"\n    mean_neutrino_pdf = np.mean(neutrino_pdf, axis=0)\n    std_pdfs = np.std(neutrino_pdf, axis=0)\n    distances = []\n\n    arr = np.logspace(-5, 0, 1000)\n\n    log_vals = np.logspace(np.log10(0.02), np.log10(0.8), 100)\n    lin_vals = np.linspace(0.1, 0.8, 10)\n    log_indices = np.searchsorted(arr, log_vals)\n    lin_indices = np.searchsorted(arr, lin_vals)\n    indices = np.concatenate((log_indices, lin_indices))\n\n    faser_pdf, x_faser = read_pdf(pdf, arr, pid, pdf_set)\n    faser_pdf = faser_pdf.flatten() * 1.16186e-09\n    faser_pdf = faser_pdf[indices]\n    mean_neutrino_pdf = mean_neutrino_pdf[indices]\n\n    std_pdfs = std_pdfs[indices]\n\n    for i in range(len(indices)):\n        if abs(mean_neutrino_pdf[i] - faser_pdf[i]) &lt; n * std_pdfs[i]:\n            distances.append(1)\n\n    distance = len(distances) / len(indices)\n\n    return distance\n</code></pre>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_bias_to_variance","title":"<code>compute_bias_to_variance(level0, level2, N_event_pred, REPLICAS)</code>","text":"<p>Compute the ratio of bias to variance in the PDF fits.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_bias_to_variance--parameters","title":"Parameters","text":"<p>level0 : Union[np.ndarray, torch.Tensor]     Baseline theory prediction. level2 : np.ndarray     Array of predictions at level 2, shape (REPLICAS, num_bins). N_event_pred : np.ndarray     Predicted events for all replicas. REPLICAS : int     Number of replicas.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_bias_to_variance--returns","title":"Returns","text":"<p>torch.Tensor     Ratio of bias to variance.</p> Source code in <code>NN_fit/postfit_measures.py</code> <pre><code>def compute_bias_to_variance(\n    self,\n    level0: Union[np.ndarray, torch.Tensor],\n    level2: np.ndarray,\n    N_event_pred: np.ndarray,\n    REPLICAS: int,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the ratio of bias to variance in the PDF fits.\n\n    Parameters\n    ----------\n    level0 : Union[np.ndarray, torch.Tensor]\n        Baseline theory prediction.\n    level2 : np.ndarray\n        Array of predictions at level 2, shape (REPLICAS, num_bins).\n    N_event_pred : np.ndarray\n        Predicted events for all replicas.\n    REPLICAS : int\n        Number of replicas.\n\n    Returns\n    -------\n    torch.Tensor\n        Ratio of bias to variance.\n    \"\"\"\n    mean_N_events = np.mean(N_event_pred, axis=0)\n    mean_N_events = torch.tensor(mean_N_events, dtype=torch.float32).view(-1)\n\n    level0 = torch.tensor(level0, dtype=torch.float32).view(-1)\n\n    diff = mean_N_events - level0\n    diffcov = torch.matmul(self.cov_matrix, diff)\n    bias = torch.dot(diff.view(-1), diffcov.view(-1))\n\n    chi_square_level2 = 0\n\n    for j in range(REPLICAS):\n        diff = mean_N_events - level2[j]\n        diff = diff.float()\n        diffcov = torch.matmul(self.cov_matrix, diff)\n        chi_square = torch.dot(diff.view(-1), diffcov.view(-1))\n\n        chi_square_level2 += chi_square\n\n    variance = chi_square_level2 / REPLICAS\n\n    ratio = bias / variance\n    return ratio\n</code></pre>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_delta_chi","title":"<code>compute_delta_chi(level0, N_event_pred, data_level1, x_vals)</code>","text":"<p>Compute the relative change in chi-squared between a baseline theory prediction and the fit prediction.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_delta_chi--parameters","title":"Parameters","text":"<p>level0 : Union[np.ndarray, torch.Tensor]     Baseline theory prediction. N_event_pred : np.ndarray     Predicted events for all replicas. data_level1 : torch.Tensor     Observed data level 1 tensor, shape (num_bins, 1). x_vals : Union[np.ndarray, torch.Tensor]     x-values (unused in this function).</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_delta_chi--returns","title":"Returns","text":"<p>torch.Tensor     Relative change in chi-squared (delta chi).</p> Source code in <code>NN_fit/postfit_measures.py</code> <pre><code>def compute_delta_chi(\n    self,\n    level0: Union[np.ndarray, torch.Tensor],\n    N_event_pred: np.ndarray,\n    data_level1: torch.Tensor,\n    x_vals: Union[np.ndarray, torch.Tensor],\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute the relative change in chi-squared between a baseline theory prediction\n    and the fit prediction.\n\n    Parameters\n    ----------\n    level0 : Union[np.ndarray, torch.Tensor]\n        Baseline theory prediction.\n    N_event_pred : np.ndarray\n        Predicted events for all replicas.\n    data_level1 : torch.Tensor\n        Observed data level 1 tensor, shape (num_bins, 1).\n    x_vals : Union[np.ndarray, torch.Tensor]\n        x-values (unused in this function).\n\n    Returns\n    -------\n    torch.Tensor\n        Relative change in chi-squared (delta chi).\n    \"\"\"\n    level0 = torch.tensor(level0, dtype=torch.float32)\n    data_level1 = data_level1.view(-1, 1)\n    diff = level0 - data_level1\n\n    diffcov = torch.matmul(self.cov_matrix, diff)\n    chi_theory = torch.dot(diff.view(-1), diffcov.view(-1))\n\n    mean_N_events = np.mean(N_event_pred, axis=0)\n    mean_N_events = torch.tensor(mean_N_events, dtype=torch.float32)\n    diff = mean_N_events - data_level1[0]\n\n    diffcov = torch.matmul(self.cov_matrix, diff)\n    chi_fit = torch.dot(diff.view(-1), diffcov.view(-1))\n\n    delta_chi = (chi_fit - chi_theory) / chi_theory\n\n    return delta_chi\n</code></pre>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_phi","title":"<code>compute_phi(data, chi_squares)</code>","text":"<p>Compute the phi metric as the difference between average chi-square over replicas and chi-square of the mean prediction.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_phi--parameters","title":"Parameters","text":"<p>data : Union[np.ndarray, torch.Tensor]     Observed data points. chi_squares : List[float]     List of chi-square losses per replica.</p>"},{"location":"api/#NN_fit.postfit_measures.Measures.compute_phi--returns","title":"Returns","text":"<p>float     The phi metric.</p> Source code in <code>NN_fit/postfit_measures.py</code> <pre><code>def compute_phi(\n    self,\n    data: Union[np.ndarray, torch.Tensor],\n    chi_squares: List[float],\n) -&gt; float:\n    \"\"\"\n    Compute the phi metric as the difference between average chi-square over replicas\n    and chi-square of the mean prediction.\n\n    Parameters\n    ----------\n    data : Union[np.ndarray, torch.Tensor]\n        Observed data points.\n    chi_squares : List[float]\n        List of chi-square losses per replica.\n\n    Returns\n    -------\n    float\n        The phi metric.\n    \"\"\"\n    num_reps = self.N_event_pred.shape[0]\n    data = torch.tensor(data, dtype=torch.float32)\n    chis = []\n\n    for i in range(num_reps):\n        N_event_rep = torch.tensor(self.N_event_pred[i], dtype=torch.float32)\n\n        diff = N_event_rep - data\n        diffcov = torch.matmul(self.cov_matrix, diff)\n        loss = torch.dot(diff.view(-1), diffcov.view(-1))\n        chis.append(loss)\n    mean_N_event_fits = np.mean(self.N_event_pred, axis=0)\n\n    # data = torch.tensor(data, dtype=torch.float32)\n    mean_N_event_fits = torch.tensor(mean_N_event_fits, dtype=torch.float32)\n\n    diff = mean_N_event_fits - data\n    diffcov = torch.matmul(self.cov_matrix, diff)\n    mean = torch.dot(diff.view(-1), diffcov.view(-1))\n\n    phi_chi = np.mean(chis) - mean\n    return phi_chi\n</code></pre>"},{"location":"api/#pullpy","title":"<code>pull.py</code>","text":""},{"location":"api/#NN_fit.pull.compute_pull","title":"<code>compute_pull(mean_pdf1, mean_pdf2, error_pdf1, error_pdf2)</code>","text":"<p>Computes pull between pdfs</p> <p>Parameters:</p> Name Type Description Default <code>mean_pdf1</code> <code>np array</code> <p>neutrino pdf 1</p> required <code>mean_pdf2</code> <code>np array</code> <p>neutrino pdf 2</p> required <code>error_pdf1</code> <code>np array</code> <p>std neutrino pdf 1</p> required <code>error_pdf2</code> <code>np array</code> <p>std neutrino pdf 2</p> required <p>Returns:</p> Name Type Description <code>list</code> <p>pull</p> Source code in <code>NN_fit/pull.py</code> <pre><code>def compute_pull(mean_pdf1, mean_pdf2, error_pdf1, error_pdf2):\n    \"\"\"Computes pull between pdfs\n\n    Args:\n        mean_pdf1 (np array): neutrino pdf 1\n        mean_pdf2 (np array): neutrino pdf 2\n        error_pdf1 (np array): std neutrino pdf 1\n        error_pdf2 (np array): std neutrino pdf 2\n\n    Returns:\n        list: pull\n    \"\"\"\n    pull = np.abs(mean_pdf1 - mean_pdf2) / np.sqrt(error_pdf1**2 - error_pdf2**2)\n    return pull\n</code></pre>"},{"location":"api/#miscellaneous-modules","title":"\ud83e\uddec Miscellaneous Modules","text":""},{"location":"api/#form_loss_fctpy","title":"<code>form_loss_fct.py</code>","text":""},{"location":"api/#NN_fit.form_loss_fct.complete_loss_fct_comb","title":"<code>complete_loss_fct_comb(pred, data, cov_matrix, f, int_point_nu, x_int, lag_mult_pos, lag_mult_int)</code>","text":"<p>Extended chi-squared loss for combined neutrino + antineutrino prediction, with constraints.</p>"},{"location":"api/#NN_fit.form_loss_fct.complete_loss_fct_comb--parameters","title":"Parameters","text":"<p>pred : torch.Tensor     Model predictions (shape: N). data : torch.Tensor     Observed pseudo-data (shape: N). cov_matrix : torch.Tensor     Covariance matrix for the data (shape: N x N). f : torch.Tensor     Raw NN output without preprocessing (shape: N). int_point_nu : torch.Tensor     Integral constraint vector (shape: N). x_int : torch.Tensor     x-values for integral constraint (shape: N). lag_mult_pos : float     Lagrange multiplier for positivity constraint. lag_mult_int : float     Lagrange multiplier for integral normalization.</p>"},{"location":"api/#NN_fit.form_loss_fct.complete_loss_fct_comb--returns","title":"Returns","text":"<p>torch.Tensor     Total loss.</p> Source code in <code>NN_fit/form_loss_fct.py</code> <pre><code>def complete_loss_fct_comb(\n    pred: torch.Tensor,\n    data: torch.Tensor,\n    cov_matrix: torch.Tensor,\n    f: torch.Tensor,\n    int_point_nu: torch.Tensor,\n    x_int: torch.Tensor,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Extended chi-squared loss for combined neutrino + antineutrino prediction, with constraints.\n\n    Parameters\n    ----------\n    pred : torch.Tensor\n        Model predictions (shape: N).\n    data : torch.Tensor\n        Observed pseudo-data (shape: N).\n    cov_matrix : torch.Tensor\n        Covariance matrix for the data (shape: N x N).\n    f : torch.Tensor\n        Raw NN output without preprocessing (shape: N).\n    int_point_nu : torch.Tensor\n        Integral constraint vector (shape: N).\n    x_int : torch.Tensor\n        x-values for integral constraint (shape: N).\n    lag_mult_pos : float\n        Lagrange multiplier for positivity constraint.\n    lag_mult_int : float\n        Lagrange multiplier for integral normalization.\n\n    Returns\n    -------\n    torch.Tensor\n        Total loss.\n    \"\"\"\n    diff = pred - data\n    diffcov = torch.matmul(cov_matrix, diff)\n\n    f = torch.where(f &gt; 0, 0, f)\n\n    loss = (\n        (1 / pred.size(0)) * torch.dot(diff.view(-1), diffcov.view(-1))\n        + abs(torch.sum(f)) * lag_mult_pos\n        + abs(torch.sum(int_point_nu * x_int)) * lag_mult_int\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#NN_fit.form_loss_fct.complete_loss_fct_nu_nub","title":"<code>complete_loss_fct_nu_nub(pred, data, cov_matrix, f, int_point_nu, int_point_nub, x_int, lag_mult_pos, lag_mult_int)</code>","text":"<p>Extended chi-squared loss for separate neutrino and antineutrino predictions, with constraints.</p>"},{"location":"api/#NN_fit.form_loss_fct.complete_loss_fct_nu_nub--parameters","title":"Parameters","text":"<p>pred : torch.Tensor     Model predictions for observed data (shape: N). data : torch.Tensor     Observed pseudo-data (shape: N). cov_matrix : torch.Tensor     Covariance matrix for the data (shape: N x N). f : torch.Tensor     Raw (non-preprocessed) neural network outputs (shape: N x 2), with:         - f[:, 0]: neutrino component (\u03bd)         - f[:, 1]: antineutrino component (\u03bd\u0304) int_point_nu : torch.Tensor     Integral constraint values for \u03bd (shape: N). int_point_nub : torch.Tensor     Integral constraint values for \u03bd\u0304 (shape: N). x_int : torch.Tensor     x-values for integral evaluation (shape: N). lag_mult_pos : float     Lagrange multiplier for enforcing positivity. lag_mult_int : float     Lagrange multiplier for enforcing normalization via integral.</p>"},{"location":"api/#NN_fit.form_loss_fct.complete_loss_fct_nu_nub--returns","title":"Returns","text":"<p>torch.Tensor     Total loss including chi-squared term, positivity penalty, and integral constraint.</p> Source code in <code>NN_fit/form_loss_fct.py</code> <pre><code>def complete_loss_fct_nu_nub(\n    pred: torch.Tensor,\n    data: torch.Tensor,\n    cov_matrix: torch.Tensor,\n    f: torch.Tensor,\n    int_point_nu: torch.Tensor,\n    int_point_nub: torch.Tensor,\n    x_int: torch.Tensor,\n    lag_mult_pos: float,\n    lag_mult_int: float,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Extended chi-squared loss for separate neutrino and antineutrino predictions, with constraints.\n\n    Parameters\n    ----------\n    pred : torch.Tensor\n        Model predictions for observed data (shape: N).\n    data : torch.Tensor\n        Observed pseudo-data (shape: N).\n    cov_matrix : torch.Tensor\n        Covariance matrix for the data (shape: N x N).\n    f : torch.Tensor\n        Raw (non-preprocessed) neural network outputs (shape: N x 2), with:\n            - f[:, 0]: neutrino component (\u03bd)\n            - f[:, 1]: antineutrino component (\u03bd\u0304)\n    int_point_nu : torch.Tensor\n        Integral constraint values for \u03bd (shape: N).\n    int_point_nub : torch.Tensor\n        Integral constraint values for \u03bd\u0304 (shape: N).\n    x_int : torch.Tensor\n        x-values for integral evaluation (shape: N).\n    lag_mult_pos : float\n        Lagrange multiplier for enforcing positivity.\n    lag_mult_int : float\n        Lagrange multiplier for enforcing normalization via integral.\n\n    Returns\n    -------\n    torch.Tensor\n        Total loss including chi-squared term, positivity penalty, and integral constraint.\n    \"\"\"\n    diff = pred - data\n    diffcov = torch.matmul(cov_matrix, diff)\n\n    fnu = f[:, 0]\n    fnub = f[:, 1]\n    fnu = torch.where(fnu &gt; 0, fnu, torch.tensor(0.0))\n    fnub = torch.where(fnub &gt; 0, fnub, torch.tensor(0.0))\n\n    loss = (\n        (1 / pred.size(0)) * torch.dot(diff.view(-1), diffcov.view(-1))\n        + abs(torch.sum(fnu)) * lag_mult_pos\n        + abs(torch.sum(fnub)) * lag_mult_pos\n        + abs(torch.sum(int_point_nu * x_int)) * lag_mult_int\n        + abs(torch.sum(int_point_nub * x_int)) * lag_mult_int\n    )\n\n    return loss\n</code></pre>"},{"location":"api/#NN_fit.form_loss_fct.raw_loss_fct","title":"<code>raw_loss_fct(pred, data, cov_matrix)</code>","text":"<p>Standard chi-squared loss without any constraints.</p>"},{"location":"api/#NN_fit.form_loss_fct.raw_loss_fct--parameters","title":"Parameters","text":"<p>pred : torch.Tensor     Model predictions (shape: N). data : torch.Tensor     Observed pseudo-data (shape: N). cov_matrix : torch.Tensor     Covariance matrix (shape: N x N).</p>"},{"location":"api/#NN_fit.form_loss_fct.raw_loss_fct--returns","title":"Returns","text":"<p>torch.Tensor     Chi-squared loss.</p> Source code in <code>NN_fit/form_loss_fct.py</code> <pre><code>def raw_loss_fct(\n    pred: torch.Tensor, data: torch.Tensor, cov_matrix: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"\n    Standard chi-squared loss without any constraints.\n\n    Parameters\n    ----------\n    pred : torch.Tensor\n        Model predictions (shape: N).\n    data : torch.Tensor\n        Observed pseudo-data (shape: N).\n    cov_matrix : torch.Tensor\n        Covariance matrix (shape: N x N).\n\n    Returns\n    -------\n    torch.Tensor\n        Chi-squared loss.\n    \"\"\"\n    diff = pred - data\n    diffcov = torch.matmul(cov_matrix, diff)\n\n    loss = (1 / pred.size(0)) * torch.dot(diff.view(-1), diffcov.view(-1))\n\n    return loss\n</code></pre>"},{"location":"api/#generate_datapy","title":"<code>generate_data.py</code>","text":""},{"location":"api/#NN_fit.generate_data.aggregate_entries_with_indices","title":"<code>aggregate_entries_with_indices(fk_tables, data, binwidths, low_bin, high_bin, threshold)</code>","text":"<p>Aggregates FK table rows and data bins until a minimum threshold of events is reached.</p> <p>This function rebins cross section data, corresponding FK table rows, and bin widths such that each new bin has at least a specified number of events (threshold). This is often necessary for achieving meaningful statistical analysis in low-statistics bins.</p>"},{"location":"api/#NN_fit.generate_data.aggregate_entries_with_indices--parameters","title":"Parameters","text":"<p>fk_tables : torch.Tensor     The FastKernel table with shape (n_bins, n_x). data : np.ndarray     Event data per bin (e.g., cross section  bin width). binwidths : np.ndarray     Width of each original bin. low_bin : np.ndarray     Lower edges of the original bins. high_bin : np.ndarray     Upper edges of the original bins. threshold : float     Minimum number of events required to form a new rebinned bin.</p>"},{"location":"api/#NN_fit.generate_data.aggregate_entries_with_indices--returns","title":"Returns","text":"<p>rebin_data : list of float     Aggregated event counts after rebinning. rebin_fk_table_mu : torch.Tensor     Rebinned FK table rows (shape: new_n_bins  n_x). rebin_binwidhts : np.ndarray     Bin widths corresponding to rebinned bins. rebin_low_bin : np.ndarray     Lower edges of the rebinned bins. rebin_high_bin : np.ndarray     Upper edges of the rebinned bins.</p>"},{"location":"api/#NN_fit.generate_data.aggregate_entries_with_indices--notes","title":"Notes","text":"<ul> <li>Remaining data after the last full threshold bin is added to the final bin.</li> <li>Bin widths are recomputed using weighted averages to ensure consistency.</li> </ul> Source code in <code>NN_fit/generate_data.py</code> <pre><code>def aggregate_entries_with_indices(\n    fk_tables: torch.Tensor,\n    data: np.ndarray,\n    binwidths: np.ndarray,\n    low_bin: np.ndarray,\n    high_bin: np.ndarray,\n    threshold: float,\n) -&gt; Tuple[\n    List[float],  # rebin_data\n    torch.Tensor,  # rebin_fk_table_mu\n    np.ndarray,  # rebin_binwidhts\n    np.ndarray,  # rebin_low_bin\n    np.ndarray,  # rebin_high_bin\n]:\n    \"\"\"\n    Aggregates FK table rows and data bins until a minimum threshold of events is reached.\n\n    This function rebins cross section data, corresponding FK table rows, and bin widths\n    such that each new bin has at least a specified number of events (threshold). This\n    is often necessary for achieving meaningful statistical analysis in low-statistics bins.\n\n    Parameters\n    ----------\n    fk_tables : torch.Tensor\n        The FastKernel table with shape (n_bins, n_x).\n    data : np.ndarray\n        Event data per bin (e.g., cross section  bin width).\n    binwidths : np.ndarray\n        Width of each original bin.\n    low_bin : np.ndarray\n        Lower edges of the original bins.\n    high_bin : np.ndarray\n        Upper edges of the original bins.\n    threshold : float\n        Minimum number of events required to form a new rebinned bin.\n\n    Returns\n    -------\n    rebin_data : list of float\n        Aggregated event counts after rebinning.\n    rebin_fk_table_mu : torch.Tensor\n        Rebinned FK table rows (shape: new_n_bins  n_x).\n    rebin_binwidhts : np.ndarray\n        Bin widths corresponding to rebinned bins.\n    rebin_low_bin : np.ndarray\n        Lower edges of the rebinned bins.\n    rebin_high_bin : np.ndarray\n        Upper edges of the rebinned bins.\n\n    Notes\n    -----\n    - Remaining data after the last full threshold bin is added to the final bin.\n    - Bin widths are recomputed using weighted averages to ensure consistency.\n    \"\"\"\n    (\n        rebin_data,\n        rebin_fk_table_mu,\n        rebin_binwidhts,\n        rebin_low_bin,\n        rebin_high_bin,\n    ) = (\n        [],\n        [],\n        [],\n        [],\n        [],\n    )\n    current_sum = 0\n    start_idx = 0\n    raw_data = data / binwidths\n\n    for i, value in enumerate(data):\n        current_sum += value\n\n        if current_sum &gt;= threshold:\n            rebin_data.append(sum(data[start_idx : i + 1]))\n            sum_binwidth = 0.0\n\n            sum_binwidth = np.sum(data[start_idx : i + 1]) / np.sum(\n                raw_data[start_idx : i + 1]\n            )\n\n            rebin_binwidhts.append(sum_binwidth)\n\n            rebin_low_bin.append(low_bin[i])\n            rebin_high_bin.append(high_bin[i])\n            summed_column_mu = torch.sum(fk_tables[start_idx : i + 1, :], axis=0)\n            summed_column_mu = summed_column_mu.unsqueeze(0)\n            rebin_fk_table_mu.append(summed_column_mu)\n\n            previous_idx = start_idx\n            start_idx = i + 1\n            current_sum = 0\n\n    if current_sum &gt;= 0:\n        rebin_data[-1] += sum(data[start_idx:])\n\n        sum_binwidth = 0.0\n\n        sum_binwidth = np.sum(data[previous_idx:]) / np.sum(raw_data[previous_idx:])\n\n        rebin_binwidhts[-1] = sum_binwidth\n\n        rebin_fk_table_mu[-1] += torch.sum(fk_tables[start_idx:, :], axis=0).unsqueeze(\n            0\n        )\n        rebin_low_bin[-1] = low_bin[previous_idx]\n        rebin_high_bin[-1] = high_bin[-1]\n\n    rebin_fk_table_mu = torch.cat(rebin_fk_table_mu, dim=0)\n\n    data = np.array(data)\n    rebin_binwidhts = np.array(rebin_binwidhts)\n    rebin_low_bin = np.array(rebin_low_bin)\n\n    return (\n        rebin_data,\n        rebin_fk_table_mu,\n        rebin_binwidhts,\n        rebin_low_bin,\n        rebin_high_bin,\n    )\n</code></pre>"},{"location":"api/#NN_fit.generate_data.compute_pseudo_data","title":"<code>compute_pseudo_data(filename_fk_mub_n, filename_fk_mub_p, filename_fk_mu_n, filename_fk_mu_p, filename_binsize, pid, pdf_name, pdf_set)</code>","text":"<p>Computes pseudo-data for neutrino and anti-neutrino scattering using FK tables and PDFs.</p> <p>This function processes FK tables (FastKernel weight tables) and convolves them with parton distribution functions (PDFs) to produce synthetic (\"pseudo\") data for both neutrinos (mu) and anti-neutrinos (mub). It also calculates associated statistical errors.</p>"},{"location":"api/#NN_fit.generate_data.compute_pseudo_data--parameters","title":"Parameters","text":"<p>filename_fk_mub_n : str     Filename for the anti-neutrino FK table (neutron target). filename_fk_mub_p : str     Filename for the anti-neutrino FK table (proton target). filename_fk_mu_n : str     Filename for the neutrino FK table (neutron target). filename_fk_mu_p : str     Filename for the neutrino FK table (proton target). filename_binsize : str     Filename containing binning information (low, high bin edges, and widths). pid : int     PDG ID of the relevant parton (e.g., 12, 14, 16 for neutrino flavors). pdf_name : str     Name of the PDF set (e.g., \"CT18\", \"NNPDF4.0\"). pdf_set : int     Index of the replica or member within the PDF set.</p>"},{"location":"api/#NN_fit.generate_data.compute_pseudo_data--returns","title":"Returns","text":"<p>data_mu : np.ndarray     Pseudo-data for neutrino cross sections, binned. data_mub : np.ndarray     Pseudo-data for anti-neutrino cross sections, binned. error_mu : np.ndarray     Statistical uncertainty (sqrt(N)) for neutrino data. error_mub : np.ndarray     Statistical uncertainty (sqrt(N)) for anti-neutrino data. fk_tables_mu : torch.Tensor     Final combined FK table for neutrinos. fk_tables_mub : torch.Tensor     Final combined FK table for anti-neutrinos. low_bin : np.ndarray     Lower bin edges of the energy bins. high_bin : np.ndarray     Upper bin edges of the energy bins. binwidths_mu : np.ndarray     Widths of bins used for neutrino integration. binwidths_mub : np.ndarray     Widths of bins used for anti-neutrino integration (same as <code>binwidths_mu</code>).</p>"},{"location":"api/#NN_fit.generate_data.compute_pseudo_data--notes","title":"Notes","text":"<ul> <li>This function uses hard-coded weights: 59.56% neutron and 40.44% proton contributions.</li> <li>Any negative or zero values in the pseudo-data are replaced with small positive values   (0.1) to avoid numerical issues.</li> <li>Requires FK tables and bin sizes to be precomputed and available as text files.</li> </ul> Source code in <code>NN_fit/generate_data.py</code> <pre><code>def compute_pseudo_data(\n    filename_fk_mub_n: str,\n    filename_fk_mub_p: str,\n    filename_fk_mu_n: str,\n    filename_fk_mu_p: str,\n    filename_binsize: str,\n    pid: int,\n    pdf_name: str,\n    pdf_set: int,\n) -&gt; Tuple[\n    np.ndarray,  # data_mu\n    np.ndarray,  # data_mub\n    np.ndarray,  # error_mu\n    np.ndarray,  # error_mub\n    torch.Tensor,  # fk_tables_mu\n    torch.Tensor,  # fk_tables_mub\n    np.ndarray,  # low_bin\n    np.ndarray,  # high_bin\n    np.ndarray,  # binwidths_mu\n    np.ndarray,  # binwidths_mub\n]:\n    \"\"\"\n    Computes pseudo-data for neutrino and anti-neutrino scattering using FK tables and PDFs.\n\n    This function processes FK tables (FastKernel weight tables) and convolves them with\n    parton distribution functions (PDFs) to produce synthetic (\"pseudo\") data for both\n    neutrinos (mu) and anti-neutrinos (mub). It also calculates associated statistical errors.\n\n    Parameters\n    ----------\n    filename_fk_mub_n : str\n        Filename for the anti-neutrino FK table (neutron target).\n    filename_fk_mub_p : str\n        Filename for the anti-neutrino FK table (proton target).\n    filename_fk_mu_n : str\n        Filename for the neutrino FK table (neutron target).\n    filename_fk_mu_p : str\n        Filename for the neutrino FK table (proton target).\n    filename_binsize : str\n        Filename containing binning information (low, high bin edges, and widths).\n    pid : int\n        PDG ID of the relevant parton (e.g., 12, 14, 16 for neutrino flavors).\n    pdf_name : str\n        Name of the PDF set (e.g., \"CT18\", \"NNPDF4.0\").\n    pdf_set : int\n        Index of the replica or member within the PDF set.\n\n    Returns\n    -------\n    data_mu : np.ndarray\n        Pseudo-data for neutrino cross sections, binned.\n    data_mub : np.ndarray\n        Pseudo-data for anti-neutrino cross sections, binned.\n    error_mu : np.ndarray\n        Statistical uncertainty (sqrt(N)) for neutrino data.\n    error_mub : np.ndarray\n        Statistical uncertainty (sqrt(N)) for anti-neutrino data.\n    fk_tables_mu : torch.Tensor\n        Final combined FK table for neutrinos.\n    fk_tables_mub : torch.Tensor\n        Final combined FK table for anti-neutrinos.\n    low_bin : np.ndarray\n        Lower bin edges of the energy bins.\n    high_bin : np.ndarray\n        Upper bin edges of the energy bins.\n    binwidths_mu : np.ndarray\n        Widths of bins used for neutrino integration.\n    binwidths_mub : np.ndarray\n        Widths of bins used for anti-neutrino integration (same as `binwidths_mu`).\n\n    Notes\n    -----\n    - This function uses hard-coded weights: 59.56% neutron and 40.44% proton contributions.\n    - Any negative or zero values in the pseudo-data are replaced with small positive values\n      (0.1) to avoid numerical issues.\n    - Requires FK tables and bin sizes to be precomputed and available as text files.\n    \"\"\"\n    x_alphas, fk_tables_mu_n = get_fk_table(\n        filename=filename_fk_mu_n, parent_dir=parent_dir\n    )\n\n    x_alphas, fk_tables_mu_p = get_fk_table(\n        filename=filename_fk_mu_p, parent_dir=parent_dir\n    )\n\n    fk_tables_mu = fk_tables_mu_n * 0.5956284 + fk_tables_mu_p * 0.4043716\n\n    x_alphas, fk_tables_mub_n = get_fk_table(\n        filename=filename_fk_mub_n, parent_dir=parent_dir\n    )\n\n    x_alphas, fk_tables_mub_p = get_fk_table(\n        filename=filename_fk_mub_p, parent_dir=parent_dir\n    )\n    fk_tables_mub = fk_tables_mub_n * 0.5956284 + fk_tables_mub_p * 0.4043716\n\n    file_path = os.path.join(parent_dir, filename_binsize)\n    low_bin, high_bin, binwidths_mu = np.loadtxt(f\"{file_path}\", unpack=True)\n    binwidths_mub = binwidths_mu\n\n    faser_pdf, x = read_pdf(pdf_name, x_alphas.detach().numpy().flatten(), pid, pdf_set)\n    faser_pdf = torch.tensor(faser_pdf, dtype=torch.float32)\n    data_mu = (\n        (torch.matmul(fk_tables_mu, faser_pdf) * binwidths_mu)\n        .detach()\n        .numpy()\n        .flatten()\n    )\n\n    faser_pdf, x = read_pdf(\n        pdf_name, x_alphas.detach().numpy().flatten(), -pid, pdf_set\n    )\n    faser_pdf = torch.tensor(faser_pdf, dtype=torch.float32)\n    data_mub = (\n        (torch.matmul(fk_tables_mub, faser_pdf) * binwidths_mub)\n        .detach()\n        .numpy()\n        .flatten()\n    )\n    data_mu = np.array(data_mu)\n    data_mub = np.array(data_mub)\n    data_mu = np.where(data_mu &lt; 0, 0, data_mu)\n    data_mu = np.where(data_mu &lt; 0, 0, data_mu)\n    data_mu = np.where(data_mu == 0, 0.1, data_mu)\n    data_mub = np.where(data_mub == 0, 0.1, data_mub)\n\n    error_mu = np.sqrt(data_mu)\n    error_mub = np.sqrt(data_mub)\n\n    return (\n        data_mu,\n        data_mub,\n        error_mu,\n        error_mub,\n        fk_tables_mu,\n        fk_tables_mub,\n        low_bin,\n        high_bin,\n        binwidths_mu,\n        binwidths_mub,\n    )\n</code></pre>"},{"location":"api/#NN_fit.generate_data.write_data","title":"<code>write_data(filename_fk_mub_n, filename_fk_mub_p, filename_fk_mu_n, filename_fk_mu_p, filename_binsize, pid, pdf_name, pdf_set, filename_to_store_events, filename_to_store_stat_error, filename_to_store_sys_error, filename_to_store_cov_matrix, min_num_events, observable, combine_nu_nub_data, multiplication_factor_sys_error)</code>","text":"<p>Computes pseudo-data from FK tables and PDFs, optionally rebins them, and writes the results to disk.</p> <p>This function is the main pipeline to produce and store pseudo-experimental data, its statistical and systematic uncertainties, covariance matrix, binning information, and FastKernel tables, all ready for use in PDF fits or phenomenology studies.</p>"},{"location":"api/#NN_fit.generate_data.write_data--parameters","title":"Parameters","text":"<p>filename_fk_mub_n : str     FK table for anti-neutrino interactions on neutrons. filename_fk_mub_p : str     FK table for anti-neutrino interactions on protons. filename_fk_mu_n : str     FK table for neutrino interactions on neutrons. filename_fk_mu_p : str     FK table for neutrino interactions on protons. filename_binsize : str     Filename for bin edges and widths (low, high, width). pid : int     PDG ID of the target parton species. pdf_name : str     Name of the LHAPDF set to use. pdf_set : int     Index of the PDF replica or member. filename_to_store_events : str     Base filename for storing rebinned event data. filename_to_store_stat_error : str     Base filename for storing statistical uncertainties. filename_to_store_sys_error : str     Base filename for storing systematic uncertainties. filename_to_store_cov_matrix : str     Base filename for storing the inverse of the covariance matrix. min_num_events : int     Minimum number of events per bin in the rebinned dataset. observable : str     Observable label (e.g., \"energy\", \"pt\") used in output filenames. combine_nu_nub_data : bool     If True, neutrino and anti-neutrino data are summed into one dataset. multiplication_factor_sys_error : float     Factor to multiply event counts for estimating systematic uncertainties.</p>"},{"location":"api/#NN_fit.generate_data.write_data--returns","title":"Returns","text":"<p>None     Writes output files directly to disk.</p>"},{"location":"api/#NN_fit.generate_data.write_data--output-files","title":"Output Files","text":"<p>../../../Data/data/:     - Re-binned event counts (combined, mu, mub)</p> <p>../../../Data/uncertainties/:     - Statistical and systematic uncertainties     - Covariance matrix (inverted, diagonal only)</p> <p>../../../Data/binning/:     - Re-binned bin edges and widths (mu, mub, or combined)</p> <p>../../../Data/fastkernel/:     - Re-binned FK tables</p>"},{"location":"api/#NN_fit.generate_data.write_data--notes","title":"Notes","text":"<ul> <li>Assumes FK and bin files are already precomputed and exist in the expected format.</li> <li>The covariance matrix is stored in inverted form (1/\u03c3\u00b2 on the diagonal).</li> <li>Output filenames are automatically labeled with PID and threshold settings.</li> <li>Handles both the case where \u03bd and \u03bd\u0304 data are stored separately or combined.</li> </ul> Source code in <code>NN_fit/generate_data.py</code> <pre><code>def write_data(\n    filename_fk_mub_n: str,\n    filename_fk_mub_p: str,\n    filename_fk_mu_n: str,\n    filename_fk_mu_p: str,\n    filename_binsize: str,\n    pid: int,\n    pdf_name: str,\n    pdf_set: int,\n    filename_to_store_events: str,\n    filename_to_store_stat_error: str,\n    filename_to_store_sys_error: str,\n    filename_to_store_cov_matrix: str,\n    min_num_events: int,\n    observable: str,\n    combine_nu_nub_data: bool,\n    multiplication_factor_sys_error: float,\n) -&gt; None:\n    \"\"\"\n    Computes pseudo-data from FK tables and PDFs, optionally rebins them, and writes the results to disk.\n\n    This function is the main pipeline to produce and store pseudo-experimental data, its\n    statistical and systematic uncertainties, covariance matrix, binning information, and\n    FastKernel tables, all ready for use in PDF fits or phenomenology studies.\n\n    Parameters\n    ----------\n    filename_fk_mub_n : str\n        FK table for anti-neutrino interactions on neutrons.\n    filename_fk_mub_p : str\n        FK table for anti-neutrino interactions on protons.\n    filename_fk_mu_n : str\n        FK table for neutrino interactions on neutrons.\n    filename_fk_mu_p : str\n        FK table for neutrino interactions on protons.\n    filename_binsize : str\n        Filename for bin edges and widths (low, high, width).\n    pid : int\n        PDG ID of the target parton species.\n    pdf_name : str\n        Name of the LHAPDF set to use.\n    pdf_set : int\n        Index of the PDF replica or member.\n    filename_to_store_events : str\n        Base filename for storing rebinned event data.\n    filename_to_store_stat_error : str\n        Base filename for storing statistical uncertainties.\n    filename_to_store_sys_error : str\n        Base filename for storing systematic uncertainties.\n    filename_to_store_cov_matrix : str\n        Base filename for storing the inverse of the covariance matrix.\n    min_num_events : int\n        Minimum number of events per bin in the rebinned dataset.\n    observable : str\n        Observable label (e.g., \"energy\", \"pt\") used in output filenames.\n    combine_nu_nub_data : bool\n        If True, neutrino and anti-neutrino data are summed into one dataset.\n    multiplication_factor_sys_error : float\n        Factor to multiply event counts for estimating systematic uncertainties.\n\n    Returns\n    -------\n    None\n        Writes output files directly to disk.\n\n    Output Files\n    ------------\n    ../../../Data/data/:\n        - Re-binned event counts (combined, mu, mub)\n\n    ../../../Data/uncertainties/:\n        - Statistical and systematic uncertainties\n        - Covariance matrix (inverted, diagonal only)\n\n    ../../../Data/binning/:\n        - Re-binned bin edges and widths (mu, mub, or combined)\n\n    ../../../Data/fastkernel/:\n        - Re-binned FK tables\n\n    Notes\n    -----\n    - Assumes FK and bin files are already precomputed and exist in the expected format.\n    - The covariance matrix is stored in inverted form (1/\u03c3\u00b2 on the diagonal).\n    - Output filenames are automatically labeled with PID and threshold settings.\n    - Handles both the case where \u03bd and \u03bd\u0304 data are stored separately or combined.\n    \"\"\"\n    (\n        data_mu,\n        data_mub,\n        error_mu,\n        error_mub,\n        fk_tables_mu,\n        fk_tables_mub,\n        low_bin,\n        high_bin,\n        binwidths_mu,\n        binwidths_mub,\n    ) = compute_pseudo_data(\n        filename_fk_mub_n,\n        filename_fk_mub_p,\n        filename_fk_mu_n,\n        filename_fk_mu_p,\n        filename_binsize,\n        pid,\n        pdf_name,\n        pdf_set,\n    )\n\n    if combine_nu_nub_data:\n        data = data_mu + data_mub\n        binwidths = binwidths_mu\n        fk_tables = fk_tables_mu + fk_tables_mub\n        (\n            data,\n            fk_tables,\n            binwidths,\n            low_bin,\n            high_bin,\n        ) = aggregate_entries_with_indices(\n            fk_tables, data, binwidths, low_bin, high_bin, min_num_events\n        )\n        stack_binning = np.column_stack((low_bin, high_bin, binwidths))\n\n        error_stat = np.sqrt(data)\n        error_sys = np.array(data) * multiplication_factor_sys_error\n\n        np.savetxt(\n            f\"../../../Data/uncertainties/{filename_to_store_stat_error}_comb_min_{min_num_events}_events_{pid}\",\n            error_stat,\n        )\n\n        np.savetxt(\n            f\"../../../Data/data/{filename_to_store_events}_comb_min_{min_num_events}_events_{pid}\",\n            data,\n        )\n        cov_matrix = np.diag(error_sys**2 + error_stat**2)\n        cov_matrix = np.linalg.inv(cov_matrix)\n        np.savetxt(\n            f\"../../../Data/uncertainties/{filename_to_store_cov_matrix}_comb_min_{min_num_events}_events_{pid}\",\n            cov_matrix,\n        )\n        np.savetxt(\n            f\"../../../Data/uncertainties/{filename_to_store_sys_error}_comb_min_{min_num_events}_events_{pid}\",\n            error_sys,\n        )\n        np.savetxt(\n            f\"../../../Data/binning/FK_{observable}_binsize_nu_min_{min_num_events}_events_{pid}\",\n            stack_binning,\n        )\n        np.savetxt(\n            f\"../../../Data/binning/FK_{observable}_binsize_nub_min_{min_num_events}_events_{pid}\",\n            stack_binning,\n        )\n        np.savetxt(\n            f\"../../../Data/fastkernel/FK_{observable}_comb_min_{min_num_events}_events_{pid}\",\n            fk_tables,\n        )\n\n    else:\n        (\n            data_mu,\n            fk_tables_mu,\n            binwidths_mu,\n            low_bin_mu,\n            high_bin_mu,\n        ) = aggregate_entries_with_indices(\n            fk_tables_mu, data_mu, binwidths_mu, low_bin, high_bin, min_num_events\n        )\n\n        (\n            data_mub,\n            fk_tables_mub,\n            binwidths_mub,\n            low_bin_mub,\n            high_bin_mub,\n        ) = aggregate_entries_with_indices(\n            fk_tables_mub, data_mub, binwidths_mub, low_bin, high_bin, min_num_events\n        )\n        error_stat_nu = np.sqrt(data_mu)\n        error_stat_nub = np.sqrt(data_mub)\n        error_sys_nu = np.array(data_mu) * multiplication_factor_sys_error\n        error_sys_nub = np.array(data_mub) * multiplication_factor_sys_error\n        stacked_data = np.hstack((data_mu, data_mub))\n        error_tot_nu = error_stat_nu**2 + error_sys_nu**2\n        error_tot_nub = error_stat_nub**2 + error_sys_nub**2\n        stacked_error = np.hstack((error_tot_nu, error_tot_nub))\n        error_stat_tot = np.hstack((error_stat_nu, error_stat_nub))\n        error_sys_tot = np.hstack((error_sys_nu, error_sys_nub))\n        np.savetxt(\n            f\"../../../Data/data/{filename_to_store_events}_nu_min_{min_num_events}_events_{pid}\",\n            data_mu,\n        )\n        np.savetxt(\n            f\"../../../Data/data/{filename_to_store_events}_nub_min_{min_num_events}_events_{-pid}\",\n            data_mub,\n        )\n        np.savetxt(\n            f\"../../../Data/data/{filename_to_store_events}_comb_min_{min_num_events}_events_{pid}\",\n            stacked_data,\n        )\n        np.savetxt(\n            f\"../../../Data/uncertainties/{filename_to_store_stat_error}_comb_min_{min_num_events}_events_{pid}\",\n            error_stat_tot,\n        )\n        np.savetxt(\n            f\"../../../Data/uncertainties/{filename_to_store_sys_error}_comb_min_{min_num_events}_events_{pid}\",\n            error_sys_tot,\n        )\n\n        cov_matrix = np.diag(stacked_error)\n        cov_matrix = np.linalg.inv(cov_matrix)\n        np.savetxt(\n            f\"../../../Data/uncertainties/{filename_to_store_cov_matrix}_comb_min_{min_num_events}_events_{pid}\",\n            cov_matrix,\n        )\n\n        np.savetxt(\n            f\"../../../Data/fastkernel/FK_{observable}_mu_min_{min_num_events}_events_{pid}\",\n            fk_tables_mu,\n        )\n        np.savetxt(\n            f\"../../../Data/fastkernel/FK_{observable}_mub_min_{min_num_events}_events_{-pid}\",\n            fk_tables_mub,\n        )\n\n        stack_binning_mu = np.column_stack((low_bin_mu, high_bin_mu, binwidths_mu))\n        np.savetxt(\n            f\"../../../Data/binning/FK_{observable}_binsize_mu_min_{min_num_events}_events_{pid}\",\n            stack_binning_mu,\n        )\n\n        stack_binning_mub = np.column_stack((low_bin_mub, high_bin_mub, binwidths_mub))\n        np.savetxt(\n            f\"../../../Data/binning/FK_{observable}_binsize_mub_min_{min_num_events}_events_{-pid}\",\n            stack_binning_mub,\n        )\n\n    print(\"The data has been written to the Data directory\")\n</code></pre>"},{"location":"api/#datayaml-fit_settingsyaml-aall_fits-runcards-etc","title":"<code>data.yaml</code>, <code>fit_settings.yaml</code>, <code>aall_fits/</code>, <code>runcards/</code>, etc.","text":"<p>These files are config/data files and are not included here directly. You can describe them in a separate \"Configuration Guide\" if needed.</p> <p>\ud83d\udcdd All modules are automatically documented from Python docstrings using mkdocstrings. Type hints, function signatures, and class hierarchies are included where available.</p>"},{"location":"framework/","title":"A brief overview of the NNfluxnu framework","text":""},{"location":"framework/#equivalence-with-nnpdf","title":"Equivalence with NNPDF","text":"<p>First of all, the framework is based on the NNPDF framework, which uses feed-forward NNs to fit PDFs from DIS structure functions. The equivalence with NNPDF is exploited in large parts of this framework.</p>"},{"location":"framework/#dis-neutrino-fluxes-and-event-rate-measurements","title":"DIS, neutrino fluxes and event rate measurements","text":"<p>The neutrinos at the LHC are decay products of forward hadron production. These hadrons are produced from pp collisions at ATLAS. The neutrinos go in the forward direction and eventually arrive ~500m downstream the FASER/SND@LHC experiments. At the FASER experiment the neutrino react via deep inelastic scattering using a Tungsten target. First of all it is important to know how the neutrino fluxes and the event rate measurements are related to each other by DIS variables and kinematic variables.</p> <p>The neutrinos react via tungsten using deep inelastic scattering like this: </p> <p>Where kinematic variables and the DIS variables are related to each other by:</p> <p>\\(E_\\nu = E_h + E_\\ell\\) </p> <p>\\(Q^2 = 4(E_h + E_\\ell)E_\\ell \\sin^2\\left(\\frac{\\theta_\\ell}{2}\\right)\\) </p> <p>\\(x = \\frac{4(E_h + E_\\ell)E_\\ell \\sin^2\\left(\\frac{\\theta_\\ell}{2}\\right)}{2m_N E_h},\\)</p> <p>In terms of these variables the charged current event rates are related to the neutrino flux by three integrals expressed in this equation:</p> <p>\\(N^{(\\nu_i)}_{\\text{int}}(x, Q^2, E_\\nu) = \\int_{Q^2_{\\text{min}}}^{Q^2_{\\text{max}}} dQ^2 \\int_{x_{\\text{min}}}^{x_{\\text{max}}} dx \\int_{E_\\nu^{\\text{(min)}}}^{E_\\nu^{\\text{(max)}}} dE_\\nu \\, \\widetilde{N}^{(\\nu_i)}_{\\text{int}}(x, Q^2, E_\\nu)\\)</p> <p>where the integrand is given by</p> <p>\\(\\widetilde{N}^{(\\nu_i)}_{\\text{int}}(x, Q^2, E_\\nu) \\equiv n_T L_T \\times \\frac{dN_{\\nu_i}(E_\\nu)}{dE_\\nu} \\times \\frac{d^2\\sigma^{\\nu_i A}(x, Q^2, E_\\nu)}{dx dQ^2} \\times \\mathcal{A}(E_\\ell, \\theta_\\ell, E_h)\\)</p> <p>where:</p> <ul> <li> <p>\\(n_T\\) and \\(L_T\\) is the atomic density of the target material (Tungsten) and the length of the detector, respectively. </p> </li> <li> <p>\\(\\frac{dN_{\\nu_i}(E_\\nu)}{dE_\\nu}\\) is the neutrino flux.</p> </li> <li> <p>\\(\\frac{d^2\\sigma^{\\nu_i A}(x, Q^2, y)}{dx dQ^2} = \\frac{G_F^2}{4\\pi x \\left(1 + Q^2/m_W^2\\right)^2} \\left[ Y_+ F_2^{\\nu p}(x, Q^2) - y^2 F_L^{\\nu p}(x, Q^2) + Y_- x F_3^{\\nu p}(x, Q^2) \\right]\\)</p> </li> <li> <p>\\(\\mathcal{A}(E_\\ell, \\theta_\\ell, E_h)\\) is an acceptance factor modelling the acceptance region of the detector.</p> </li> </ul>"},{"location":"framework/#fast-kernel-tables-computational-efficiency","title":"Fast-Kernel tables: computational efficiency","text":"<p>As can be seen from the equations above, the flux and the event rates are related to each other through several integrals. When trying to parametrise a neutrino flux using an NN this is very computationally expensive, since every new try for the flux results in having to do several integrals. This is why Fast-Kernel are used in this framework to replace the convolutions by a single matrix multiplication. The idea is to take the neutrino flux outside of the integral by expressing it in terms of al inear expansion using Lagrange polynomials. In order to arrive at this result, first the neutrino PDF is defined as :</p> <p>\\(f_{\\nu_i} = \\frac{s_{pp}}{2} \\frac{dN_{\\nu_i}(E_\\nu)}{dE_\\nu}, \\quad i = e, \\nu, \\tau\\)</p> <p>Where \\(x_\\nu\\) is the momentum fraction and is defined as:</p> <p>\\(x_\\nu = \\frac{2 E_\\nu}{s_{pp}}, \\quad 0 \\leq x_\\nu \\leq 1\\)</p> <p>and where \\(s_{pp}\\) is the center of mass from the proton-proton collision. </p> <p>The neutrino PDF is then expressed in a linear expansion in an interpolation basis:</p> <p>\\(f_{\\nu_i}(x_\\nu) \\simeq \\sum_{\\alpha=1}^{n_x} f_{\\nu_i}(x_{\\nu,\\alpha}) I_\\alpha(x_\\nu)\\)</p> <p>Where \\(I_\\alpha(x_\\nu)\\) are Lagrange polynomials</p> <p>Then the number of events are related to the neutrino PDF as follows:</p> <p>\\(N^{(\\nu_i)}_{\\text{int}}(E_\\nu) = \\sum_{\\alpha=1}^{n_x} f_{\\nu_i}(x_{\\nu,\\alpha}) \\int_{E_\\nu^{\\text{(min)}}}^{E_\\nu^{\\text{(max)}}} dE_\\nu  \\int_{Q^2_{\\alpha}=1\\,\\text{GeV}^2}^{2m_N E_\\nu} dQ^2  \\int_{Q^2/2m_N E_\\nu}^{1} dx \\left[ n_T L_T \\times \\frac{2}{\\sqrt{s_{pp}}} I_\\alpha(x_\\nu) \\times \\left. \\frac{d^2\\sigma^{\\nu_i A}_{\\text{NLO+PS}}(x, Q^2, E_\\nu)}{dx dQ^2} \\right|_{\\text{fid}} \\right]\\)</p> <p>And the FK-table is given by</p> <p>\\(\\text{FK}_{\\alpha,j} \\equiv \\frac{2 n_T L_T}{\\sqrt{s_{pp}}} \\int_{E_\\nu^{\\text{(min)}}}^{E_\\nu^{\\text{(max)}}} dE_\\nu  \\int_{Q_0^2}^{2m_N E_\\nu} dQ^2  \\int_{Q^2 / 2m_N E_\\nu}^{1} dx \\left[ I_\\alpha(x_\\nu) \\frac{d^2\\sigma^{\\nu_i A}_{\\text{NLO+PS}}(x, Q^2, E_\\nu)}{dx dQ^2} \\right]_{\\text{fid}}\\)</p> <p>And the number of scattering events, the fk-table and the neutrino PDF are then related to each other as follows:</p> <p>\\(N^{(\\nu_i)}_{\\text{int}}(E_\\nu, j) = \\sum_{\\alpha=1}^{n_x} f_{\\nu_i}(x_{\\nu,\\alpha}) \\text{FK}_{\\alpha,j}\\)</p> <p>A schematic picture of what happens is displayed below:</p> <p></p>"},{"location":"framework/#the-machine-learning-model","title":"The Machine Learning model","text":"<p>The structure of the machine learning model looks like:</p> <p></p> <p>A preprocessing function is imposed to increase computational efficiency, smooth the paramterisation and impose non-divergent behaviour at low and high x. The NN used in the framework contains a few hidden layers with a few nodes. The loss is defined as:</p> <p>\\(chi^2 = \\sum_{i,j}^{N_{\\text{dat}}} (D - P)_i \\, C^{-1}_{ij} \\, (D - P)_j,\\)</p> <p>There is also an option to extend the loss to also make sure the neutrino PDF is positive definite when this is not imposed by the activation functions.</p> <p>When training the NN, a so-called stopping algorithm is employed, which looks like: </p> <p>The idea is to stop training when the loss stops decreasing with a number of patience epochs to prevent the training from stopping too early.</p> <p>As is the case in NNPDF, the uncertainties on the neutrino PDFs are computed by running so-called closure tests. Using the errors on the data, N Monte Carlo replicas are produced and fitted. Then, the mean and standard deviation is computed using N neutirno PDFs from these N fits. </p> <p>The specific parameters used in this work, like the activation function and the optimizer can be found in the fit_settings.yaml file. </p>"},{"location":"framework/#hyperoptimization-algorithm","title":"Hyperoptimization algorithm","text":"<p>An hyperparameter optimization algorithm is also available. The idea behind this algorithm is generality i.e. the methodology should work for all datasets, provided the hyperparameters chosen are sensible. The algorithm in this work uses k-fold cross validation and bayesian optimization to find the best set of hyperparameters. The idea behind k-fold cross validation is to fit the entire dataset consisting of k-folds, except for one. This is then iterated until all possible partitions have been fitted. The hyperparameters which produced the lowest mean loss is then used to run the fit. Bayesian optimization is employed to find this minimum.</p>"},{"location":"framework/#postfit-analysis","title":"Postfit analysis","text":"<p>The postfit analysis consists of two parts: postift criteria and postfit measures. The first one checks if there is a certain fitted neutrino PDF which did not converge. For example, if the \\(\\chi^2\\) from a particular replica deviates more than 4\\(\\sigma\\) from the mean \\(\\chi^2\\) computed using all the replicas, this replica is discarded. The postfit measures are used to assess the quality of the fit using several statistical measures, such as computing the degree to which the fit has been an overfit/underfit. </p>"},{"location":"usage/","title":"How to use the code","text":"<p>In this section a guide on how to use the code is provided including examples.</p>"},{"location":"usage/#data-needed-to-run-a-fit","title":"Data needed to run a fit","text":"<p>First of all, one needs data to run a fit. More specifically one needs:</p> <ul> <li> <p>FK-tables</p> </li> <li> <p>Binwidths</p> </li> <li> <p>Event rates</p> </li> <li> <p>Errors</p> </li> <li> <p>Grid nodes</p> </li> </ul> <p>The event rates and errors can either be from event rate measurements or can be sourced from pseudo data. It is also possible to create pseudo data and to rebin the data to a certain number of events if one has: </p> <ul> <li> <p>FK-tables</p> </li> <li> <p>Binwidths</p> </li> <li> <p>Neutrino flux</p> </li> </ul> <p>Using the file <code>generate_data.py</code> one can generate data, rebin it if wanted and write the data to files stored in the Data directory. This data is pseudo data and one needs an input neutrino flux with which event rates can be computed by convoluting this with the FK-table.</p> <p>All settings for the data generation can be specified in a yaml file like this: <pre><code>data:\n  pdf: \"FASERv_Run3_EPOS+POWHEG_7TeV\" \n  min_num_events: 20\n  observable: \"Eh\"\n  combine_nu_nub_data: False\n  particle_id: 14\n  pdf_set: 2\n  filename_fk_table: \"FK_Eh_final\"\n  filename_binwidth: \"FK_Eh_binsize\"\n  filename_to_store_events: \"FASERv_Run3_EPOS+POWHEG_7TeV_events\"\n  filename_to_store_stat_error: \"FASERv_Run3_EPOS+POWHEG_7TeV_stat_error\"\n  filename_to_store_sys_error: \"FASERv_Run3_EPOS+POWHEG_7TeV_sys_error\"\n  filename_to_store_cov_matrix: \"FASERv_Run3_EPOS+POWHEG_7TeV_cov_matrix\"\n  multiplication_factor_sys_error: 0.2\n</code></pre> where <code>multiplication_factor_sys_error</code> is a factor to take pseudo systematic uncertainties into account. One can put it to 0 if one only wants to include statistical uncertainties.  Then type: <pre><code>python generate_data.py data.yaml\n</code></pre> to generate data</p> <p>All the data files should be written to and read from the Data directory.</p>"},{"location":"usage/#fk-table-generation-with-powhegpythia8","title":"FK-table generation with POWHEG+PYTHIA8","text":"<p>The FK-tables can be generated by using the modified version of the neutrino DIS Monte Carlo event generator. This variant replaces the neutrino flux with the set of Lagrange interpolation polynomials following the procedure described here. To generate the FK-table for an observable a histogram has to be booked and filled in the analysis subroutine. After the simulation a differential distribution will be present for each member of the basis of interpolation polynomials with the same binning. The spacing of the grids, blocksize and the dimension of the basis of interpolation polynomials can be adapted in the by modifying the subroutines <code>interpolation.f90</code> and <code>lepton_flux.f90</code>. To compile the code, adapt the paths in the Makefile. An example for a runcard and scripts to run the code are provided in the testrun-fk folder.</p>"},{"location":"usage/#available-data-and-format","title":"Available Data and Format","text":"<p>In the <code>Data</code> directory of the git repository, all data used in this work is available: FK-tables, binning, event rates and statistical uncertainties. The filenames of this data is as follows: </p> <p><code>datatype_observable_(fine)_geometry_generator_7TeV_nu(mu,bmu,e,be)_W.dat</code></p> <p>or </p> <p><code>datatype_observable_(fine)_geometry_generator_7TeV_comb_W.dat</code></p> <p>The corresponding fluxes are formatted in this way:</p> <p><code>geometry_(generator/bsm/IC)_7TeV.dat</code></p> <p>This data was used to parametrise the neutrino fluxes which can be found in the <code>neutrino_pdfs_lhpadf</code> folder. The user can also use this data to make fits.</p>"},{"location":"usage/#running-a-fit","title":"Running a fit","text":"<p>When one wants to run a fit it starts with a yaml file. In this file all settings are found, for example the structure of the NN, the data one wants to use and the training parameters:</p> <p><pre><code>model:\n  hidden_layers: [4, 4,4]\n  activation_function: [\"softplus\",\"softplus\",\"softplus\"]\n  preproc: True\n  extended_loss: False\n  num_output_layers: 1\n  num_input_layers: 1\n\nclosure_test:\n  fit_level: 2\n  num_reps: 3\n  diff_l1_inst: 3\n\ntraining:\n  patience: 100\n  max_epochs: 2500\n  lr: 0.03\n  optimizer: \"Adam\"\n  wd: 0.001\n  range_alpha: 5\n  range_beta: 20\n  range_gamma: 100\n  validation_split: 0.0\n  max_chi_sq: 5\n  lag_mult_pos: 0.001\n  lag_mult_int: 0.001\n  x_int: [0.001,0.98]\n\ndataset:\n  observable: \"Eh\"\n  filename_data: \"FASERv_Run3_EPOS+POWHEG_7TeV_events_comb_min_20_events\"\n  filename_stat_error: \"FASERv_Run3_EPOS+POWHEG_7TeV_stat_error_comb_min_20_events\"\n  filename_sys_error: \"FASERv_Run3_EPOS+POWHEG_7TeV_sys_error_comb_min_20_events\"\n  filename_cov_matrix: \"FASERv_Run3_EPOS+POWHEG_7TeV_cov_matrix_comb_min_20_events\"\n  filename_binning: \"FK_Eh_binsize_nub_min_20_events\"\n  grid_node: 'x_alpha.dat'\n  pdf: \"FASERv_Run3_EPOS+POWHEG_7TeV\"\n  pdf_set: 2\n  fit_faser_data: False\n\npostfit:\n  postfit_criteria: True\n  postfit_measures: True\n  dir_for_data: 'test_dir_faserv_Eh_elec_epos'\n  neutrino_pdf_fit_name_lhapdf: 'testgrid'\n  particle_id_nu: 12\n  particle_id_nub: -12\n  produce_plot: True\n</code></pre> If <code>extended_loss</code> is set to True one also takes positivity into account as well as ensures the neutrino PDF goes to zero in low- and high-x regions. The lag_mult_pos, lag_mult_int and x_int are the settings for this extended loss i.e. the Lagrange multipliers and the x-points to punish high-values of the neutrino PDF. If fit_faser_data is set o True the bins with the highest energy for muon and anti-muon neutrino event rates are combined due to the way FASER has measured and published the event rate measurements.</p> <p>When running a fit type:  <pre><code>python execute_fit.py runcards/fit_settings.yaml\n</code></pre> This will perform the fit and also, if wanted, perform the postfit analysis consisting of postfit measures, postfit criteria and plot the result. It will also write the results to a seperate directory and to a separate LHAPDF grid. </p>"},{"location":"usage/#hyperparameter-optimization","title":"Hyperparameter optimization","text":"<p>An hyperparameter optimization algorithm is also available, based on k-fold cross validation and bayesian optimization. To perform hyperparameter optimizationf for a specific dataset run:</p> <p><pre><code>python perform_hyperopt.py hyperopt_settings.py\n</code></pre> In the Framework section, the workings of this algorithm will be explained. </p>"}]}